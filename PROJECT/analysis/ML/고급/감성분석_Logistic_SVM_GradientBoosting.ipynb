{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a14a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ“¦ 1. ë°ì´í„° ë¡œë”© ë° ë¶„í• \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"36000_reviews_label.csv\", encoding=\"utf-8-sig\")\n",
    "df = df[['sentence', 'label']].dropna()\n",
    "df = df[df['label'].isin(['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½'])]\n",
    "\n",
    "X = df['sentence'].fillna('')\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21370ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ› ï¸ 2. í˜•íƒœì†Œ ë¶„ì„ê¸° ë° ë¶ˆìš©ì–´ ì •ì˜\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = set([\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜', 'ë¡œ', 'ìœ¼ë¡œ',\n",
    "    'ì™€', 'ê³¼', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ê°™ì´', 'ê¹Œì§€', 'ë§ˆì €', 'ì¡°ì°¨', 'ë¶€í„°',\n",
    "    'ì´ë‚˜', 'ë‚˜', 'ì´ë©°', 'ë©°', 'ë“±', 'í•˜ë‹¤', 'í•œë‹¤', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´',\n",
    "    'ë˜ì–´', 'ë˜ë‹¤', 'ë˜ê³ ', 'ë˜ë‹ˆ', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ã…‚ë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ë‹¤', 'ë°©ì´', 'ì œëŒ€ë¡œ',\n",
    "    'ê³ ', 'ë©´', 'ê²Œ', 'ì§€', 'ì£ ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ',\n",
    "    'ë˜í•œ', 'ë˜ëŠ”', 'ë°', 'ì¦‰', 'í•œí¸', 'ë°˜ë©´ì—', 'ê·¼ë°', 'ì¢€', 'ë„ˆë¬´', 'ì •ë§', 'ë§ì´',\n",
    "    'ì•„', 'ì–´', 'ì˜ˆ', 'ì‘', 'ë„¤', 'ì•ˆ', 'ì•Šë‹¤', 'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ì´ë‹¤'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08276ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ§¼ 3. tokenizer + ë²¡í„°í™”\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [\n",
    "        word for word, pos in okt.pos(text, stem=True)\n",
    "        if pos in ['Noun', 'Adjective'] and word not in stopwords and len(word) > 1\n",
    "    ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac730be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ” 4. Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logi = LogisticRegression(max_iter=1000, class_weight='balanced', multi_class='multinomial', solver='lbfgs')\n",
    "logi.fit(X_train_vec, y_train)\n",
    "logi_pred = logi.predict(X_test_vec)\n",
    "\n",
    "print(\"ğŸ“Œ Logistic Regression ê²°ê³¼:\")\n",
    "print(classification_report(y_test, logi_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ff664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ” 5. SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(class_weight='balanced', C=1.0, max_iter=1000)\n",
    "svm.fit(X_train_vec, y_train)\n",
    "svm_pred = svm.predict(X_test_vec)\n",
    "\n",
    "print(\"ğŸ“Œ SVM ê²°ê³¼:\")\n",
    "print(classification_report(y_test, svm_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ” 6. Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gbc.fit(X_train_vec.toarray(), y_train)\n",
    "gbc_pred = gbc.predict(X_test_vec.toarray())\n",
    "\n",
    "print(\"ğŸ“Œ Gradient Boosting ê²°ê³¼:\")\n",
    "print(classification_report(y_test, gbc_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ¨ 7. í˜¼ë™ í–‰ë ¬ ì‹œê°í™” (for GBC)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import font_manager as fm\n",
    "import numpy as np\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/NanumGothic.ttf\"\n",
    "font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "plt.rc('font', family=font_name)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "labels = sorted(df['label'].unique())\n",
    "cm = confusion_matrix(y_test, gbc_pred, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Oranges)\n",
    "plt.title(\"Gradient Boosting í˜¼ë™ í–‰ë ¬\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ğŸ“Š 8. ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸ (GBC ê¸°ì¤€)\n",
    "report = classification_report(y_test, gbc_pred, output_dict=True)\n",
    "import pandas as pd\n",
    "\n",
    "df_report = pd.DataFrame(report).transpose().iloc[:-3]\n",
    "df_report[['precision', 'recall', 'f1-score']].plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"Gradient Boosting ì„±ëŠ¥ ì§€í‘œ ë¹„êµ\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}