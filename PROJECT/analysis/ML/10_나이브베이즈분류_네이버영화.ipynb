{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e9c125",
   "metadata": {},
   "source": [
    "## 긍정, 부정의 감성분석 : 데이터 양을 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc327b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 파일을 병합하기\n",
    "df_train = pd.read_table('ratings_train.txt')\n",
    "df_test = pd.read_table('ratings_test.txt')\n",
    "\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060aa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "df.drop_duplicates(subset=['document'], inplace=True)\n",
    "# 결측치 제거\n",
    "df = df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8570c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복과 결측치를 제거한 데이터를 파일로 저장하기\n",
    "df.to_csv('naver_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"naver_filtered.csv\")\n",
    "# 데이터를 라벨 0,1의 비중을 맞춰서 1000를 추출\n",
    "\n",
    "df = df[['document','label']]\n",
    "\n",
    "df_pos = df[df['label'] == 1]\n",
    "df_neg = df[df['label'] == 0]\n",
    "\n",
    "sample_pos = df_pos.sample(n=1500, random_state=12)\n",
    "sample_neg = df_neg.sample(n=1500, random_state=12)\n",
    "\n",
    "sample_df = pd.concat([sample_pos,sample_neg], ignore_index=True)\n",
    "\n",
    "print(\"샘플링된 데이터 개수 : \", len(sample_df))\n",
    "print(\"라벨별 데이터 개수 : \\n\", sample_df['label'].value_counts())\n",
    "\n",
    "df = sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 형태소 분석기를 통해, 영화 리뷰 문장들을 전처리\n",
    "# 한글만 남기고 모두 제거하기\n",
    "df['document'] = df['document'].str.replace(\"[^가-힣 ]\",\"\",regex=True)\n",
    "# '빈칸'만 남은 데이터는 공백으로 변경\n",
    "df['document'] = df['document'].str.replace(\"^ +\",\"\",regex=True)\n",
    "# 공백만 있는 데이터는 Null / Nan으로 변경\n",
    "import numpy as np\n",
    "df['document'] = df['document'].replace(\"\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df에서 null 제거\n",
    "df = df.dropna(how='any')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "stopwords =  ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '.']\n",
    "\n",
    "# 훈련 데이터 만들기\n",
    "train_data = []\n",
    "for i in range(len(df)):\n",
    "    emotion = df.iloc[i]['label']\n",
    "    text = df.iloc[i]['document']\n",
    "    # 문장을 형태소 분석하기 -> 문장을 단어(토큰)의 리스트로 변환 \n",
    "    tokenized_text = okt.morphs(text)\n",
    "    # 형태소 분석결과에서 불용어를 제거함\n",
    "    filtered_tokens = [ word for word in tokenized_text if not word in stopwords ]\n",
    "    # (토큰화 된 문장, 감정) 형태의 튜플을 리스트에 저장\n",
    "    train_data.append((filtered_tokens,emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for tokens, emotion in train_data :\n",
    "    for token in tokens :\n",
    "        all_tokens.append(token)\n",
    "vocabulary = list(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_set(tokens, vocabulary) :\n",
    "    token_set = set(tokens)\n",
    "    features = { word : (word in token_set) for word in vocabulary }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b39377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_datas = []\n",
    "train_feature_datas = [\n",
    "    (create_feature_set(tokens,vocabulary),emotion)\n",
    "    for tokens, emotion in train_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "model = nltk.NaiveBayesClassifier.train(train_feature_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9dac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, model, tokenizer, vocabulary) :\n",
    "    tokens = tokenizer.morphs(text)\n",
    "    filtered_tokens = [ word for word in tokens if not word in stopwords ]\n",
    "    features = create_feature_set(filtered_tokens, vocabulary)\n",
    "    clasified_label = model.classify(features)\n",
    "    print(model.prob_classify(features).prob(clasified_label))\n",
    "    result = \"부정\" if clasified_label == 0 else \"긍정\"\n",
    "    print(\"예측 결과 : \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4592ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"새로운 직장 동료들이 다들 좋은 사람들이라 편하게 적응하고 있어\",   #기쁨\n",
    "    \"첫 발표인데, 갑자기 노트북이 꺼져서 정말 당황스러웠어\",            #당황\n",
    "    \"상사가 자기가 할 일을 나에게 다 떠넘겨서 너무 화가 나\",            #분노\n",
    "    \"내일 중요한 면접이 있는데 하나도 준비를 못 해서 너무 걱정돼\",      #불안\n",
    "    \"믿었던 친구에게 배신당해서 큰 상처를 받았어\",                      #상처\n",
    "    \"열심히 준비한 프로젝트에서 해고 통보를 받으니 너무 슬프다\"         #슬픔\n",
    "]\n",
    "for text in text_list :\n",
    "    classify_text(text, model, okt, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4473b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"오늘 본 영화가 정말 감동적이었고, 강아지가 너무 귀여웠어.\",        #기쁨\n",
    "    \"친구가 갑자기 내 생일 파티를 열어줘서 깜짝 놀랐어!\",               #당황\n",
    "    \"밤새 모기가 윙윙거려서 잠을 설쳤더니 짜증 나.\",                    #분노\n",
    "    \"내일 소개팅하는데 무슨 말을 해야 할지 모르겠어.\",                  #불안\n",
    "    \"다이어트에 또 실패해서 속상해.\",                                   #상처\n",
    "    \"키우던 반려 식물이 시들어버려서 마음이 아파.\"                      #슬픔\n",
    "]\n",
    "for text in text_list :\n",
    "    classify_text(text, model, okt, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\n",
    "    \"이 영화 진짜 역대급 꿀잼! 배우들 연기도 최고였어요.\",\n",
    "    \"시간이 아깝다... 스토리가 너무 지루하고 재미없어요\",\n",
    "    \"그냥 평범한 영화, 볼만은 한데 추천할 정도는 아님\"\n",
    "]\n",
    "for text in text_list :\n",
    "    classify_text(text, model, okt, vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
