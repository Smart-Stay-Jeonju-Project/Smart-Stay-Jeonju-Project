{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e9c125",
   "metadata": {},
   "source": [
    "## 긍정, 부정의 감성분석 : 데이터 양을 늘리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc327b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 파일을 병합하기\n",
    "df_train = pd.read_table('ratings_train.txt')\n",
    "df_test = pd.read_table('ratings_test.txt')\n",
    "\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060aa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "df.drop_duplicates(subset=['document'], inplace=True)\n",
    "# 결측치 제거\n",
    "df = df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8570c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복과 결측치를 제거한 데이터를 파일로 저장하기\n",
    "df.to_csv('naver_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab8d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링된 데이터 개수 :  6000\n",
      "라벨별 데이터 개수 : \n",
      " label\n",
      "1    3000\n",
      "0    3000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"naver_filtered.csv\")\n",
    "# 데이터를 라벨 0,1의 비중을 맞춰서 1000를 추출\n",
    "\n",
    "df = df[['document','label']]\n",
    "\n",
    "df_pos = df[df['label'] == 1]\n",
    "df_neg = df[df['label'] == 0]\n",
    "\n",
    "sample_pos = df_pos.sample(n=3000, random_state=11)\n",
    "sample_neg = df_neg.sample(n=3000, random_state=11)\n",
    "\n",
    "sample_df = pd.concat([sample_pos,sample_neg], ignore_index=True)\n",
    "\n",
    "print(\"샘플링된 데이터 개수 : \", len(sample_df))\n",
    "print(\"라벨별 데이터 개수 : \\n\", sample_df['label'].value_counts())\n",
    "\n",
    "df = sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5cd6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 형태소 분석기를 통해, 영화 리뷰 문장들을 전처리\n",
    "# 한글만 남기고 모두 제거하기\n",
    "df['document'] = df['document'].str.replace(\"[^가-힣 ]\",\"\",regex=True)\n",
    "# '빈칸'만 남은 데이터는 공백으로 변경\n",
    "df['document'] = df['document'].str.replace(\"^ +\",\"\",regex=True)\n",
    "# 공백만 있는 데이터는 Null / Nan으로 변경\n",
    "import numpy as np\n",
    "df['document'] = df['document'].replace(\"\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54ad0340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5963 entries, 0 to 5999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   document  5963 non-null   object\n",
      " 1   label     5963 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 139.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# df에서 null 제거\n",
    "df = df.dropna(how='any')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc626edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', \n",
    "    '고', '면', '며', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', \n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '일', '문제', '경우', '부분',\n",
    "    '내용', '결과', '자체', '가지',\n",
    "    '뿐', '대로', '만큼', '만', '지', '따름', '나름', '김에', '터', '너무', '어요'\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b347b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "# 훈련 데이터 만들기\n",
    "train_data = []\n",
    "for i in range(len(df)):\n",
    "    emotion = df.iloc[i]['label']\n",
    "    text = df.iloc[i]['document']\n",
    "    # 문장을 형태소 분석하기 -> 문장을 단어(토큰)의 리스트로 변환 \n",
    "    tokenized_text = okt.morphs(text)\n",
    "    # 형태소 분석결과에서 불용어를 제거함\n",
    "    filtered_tokens = [ word for word in tokenized_text if not word in stopwords ]\n",
    "    # (토큰화 된 문장, 감정) 형태의 튜플을 리스트에 저장\n",
    "    train_data.append((filtered_tokens,emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7089eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for tokens, emotion in train_data :\n",
    "    for token in tokens :\n",
    "        all_tokens.append(token)\n",
    "vocabulary = list(set(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffd5f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_set(tokens, vocabulary) :\n",
    "    token_set = set(tokens)\n",
    "    features = { word : (word in token_set) for word in vocabulary }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b39377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_datas = []\n",
    "train_feature_datas = [\n",
    "    (create_feature_set(tokens,vocabulary),emotion)\n",
    "    for tokens, emotion in train_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "130b25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "model = nltk.NaiveBayesClassifier.train(train_feature_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a9dac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, model, tokenizer, vocabulary) :\n",
    "    tokens = tokenizer.morphs(text)\n",
    "    filtered_tokens = [ word for word in tokens if not word in stopwords ]\n",
    "    features = create_feature_set(filtered_tokens, vocabulary)\n",
    "    clasified_label = model.classify(features)\n",
    "    print(model.prob_classify(features).prob(clasified_label))\n",
    "    result = \"부정\" if clasified_label == 0 else \"긍정\"\n",
    "    print(\"예측 결과 : \", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed02dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6049486528845978\n",
      "예측 결과 :  긍정\n",
      "0.7208121839324312\n",
      "예측 결과 :  긍정\n",
      "0.8881156728313495\n",
      "예측 결과 :  부정\n",
      "0.8311352534564015\n",
      "예측 결과 :  부정\n",
      "0.9544228812571377\n",
      "예측 결과 :  긍정\n",
      "0.969752393579962\n",
      "예측 결과 :  긍정\n",
      "0.9803166711836618\n",
      "예측 결과 :  긍정\n",
      "0.7723642590640472\n",
      "예측 결과 :  긍정\n",
      "0.9117733898121869\n",
      "예측 결과 :  긍정\n",
      "0.7245181372735994\n",
      "예측 결과 :  부정\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"깨긋하구 아늑한분위기\",\n",
    "    \"조용하고 주위에 볼거리도 많아요\",\n",
    "    \"방이 작고 방음 안되요 주차장 찾기 어려워요\",\n",
    "    \"3명이상 숙박하기에는 좋지 않습니다\",\n",
    "    \"사장님도 친절하시고 숙소 안도 따뜻해서 좋았어요\",\n",
    "    \"친절하고 깔끔해서 지내기 좋았습니다\",\n",
    "    \"연박으로 묵었는데 크리스마스 너무 잘 보냈습니다 사장님 친절하시고 시설도 괜찮습니다\",\n",
    "    \"친절하시고 위치도 경기전 근처라 한옥마을 다니기도 편했습니다\",\n",
    "    \"딱 1박용으로 괜찮은 곳이에요\",\n",
    "    \"특별한 서비스는 없었어요\"\n",
    "]\n",
    "df = pd.read_csv('1_1400_output_partial.csv')\n",
    "\n",
    "for text in df['sentence'] :\n",
    "    classify_text(text, model, okt, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3067ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 모델 저장\n",
    "with open(\"naive_bayes_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# 필요한 경우 단어 사전(vocabulary)도 같이 저장\n",
    "with open(\"vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
