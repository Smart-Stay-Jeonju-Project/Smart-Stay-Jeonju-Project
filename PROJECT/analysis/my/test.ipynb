{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d78f5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 📊 Confusion Matrix ===\n",
      "[[196  31]\n",
      " [105 167]]\n",
      "\n",
      "=== 🧾 Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.651     0.863     0.742       227\n",
      "           1      0.843     0.614     0.711       272\n",
      "\n",
      "    accuracy                          0.727       499\n",
      "   macro avg      0.747     0.739     0.727       499\n",
      "weighted avg      0.756     0.727     0.725       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. CSV 파일 불러오기\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. 벡터화 (BoW 방식, 단순 단어 단위 토크나이징)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== 📊 Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== 🧾 Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', \n",
    "    '고', '면', '며', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', \n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '일', '문제', '경우', '부분', \n",
    "    '내용', '결과', '자체', '가지',\n",
    "    '뿐', '대로', '만큼', '만', '지', '따름', '나름', '김에', '터', '너무', '어요'\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ca024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== TF-IDF Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== TF-IDF Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ebefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1~2그램 적용 (ex: \"맛있어요\", \"정말 맛있어요\")\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ngram_vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test_ng = ngram_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_ng, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_ng = model.predict(X_test_ng)\n",
    "\n",
    "print(\"=== N-gram Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_ng))\n",
    "print(\"\\n=== N-gram Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_ng, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + N-gram (1~2그램)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d265904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train_df = pd.read_csv(\"ratings_train_extended.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test_extended.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. TF-IDF 벡터화 + N-gram(1,2)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측 및 성능 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# 5. 중요 단어 추출\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_  # shape: (3, n_features)\n",
    "\n",
    "# 클래스별 상위 단어 추출\n",
    "topn = 10\n",
    "weights = {}\n",
    "for idx, class_label in enumerate(model.classes_):  # -1, 0, 1\n",
    "    top_indices = np.argsort(coef[idx])[::-1][:topn]\n",
    "    weights[class_label] = pd.DataFrame({\n",
    "        'word': feature_names[top_indices],\n",
    "        'weight': coef[idx][top_indices]\n",
    "    })\n",
    "\n",
    "# 6. 시각화 준비용 데이터프레임 생성\n",
    "df_plot = pd.concat(\n",
    "    [df.assign(label=str(label)) for label, df in weights.items()],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# 7. 시각화: 클래스별 단어 중요도\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Mac이라면 AppleGothic, Windows면 Malgun Gothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "labels = ['-1 (부정)', '0 (중립)', '1 (긍정)']\n",
    "color_dict = {'-1': '#e74c3c', '0': '#f1c40f', '1': '#2ecc71'}\n",
    "\n",
    "for i, label in enumerate(['-1', '0', '1']):\n",
    "    subset = df_plot[df_plot['label'] == label]\n",
    "    sns.barplot(\n",
    "        ax=axes[i],\n",
    "        data=subset,\n",
    "        y='word',\n",
    "        x='weight',\n",
    "        color=color_dict[label]\n",
    "    )\n",
    "    axes[i].set_title(f\"감성 {labels[i]} 상위 단어\")\n",
    "    axes[i].set_xlabel(\"가중치(weight)\")\n",
    "    axes[i].set_ylabel(\"단어\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드용 단어 + 가중치 딕셔너리 만들기\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "\n",
    "# 워드클라우드 그리기 함수\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# 클래스별 워드클라우드 출력\n",
    "draw_wordcloud(word_weights[-1], '부정 감성 주요 단어', 'Reds')\n",
    "draw_wordcloud(word_weights[0], '중립 감성 주요 단어', 'Oranges')\n",
    "draw_wordcloud(word_weights[1], '긍정 감성 주요 단어', 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 문장 리스트\n",
    "new_texts = [\n",
    "    \"침대가 너무 불편해서 허리가 아팠어요.\",\n",
    "    \"친절하고 조용해서 편하게 쉬었어요.\",\n",
    "    \"그냥 무난한 숙소였어요. 특별한 건 없었어요.\"\n",
    "]\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(new_texts)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "# 결과 출력\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측 감성: {predictions[i]} (확률: {probs[i]})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
