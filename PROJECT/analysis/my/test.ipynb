{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d78f5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ“Š Confusion Matrix ===\n",
      "[[196  31]\n",
      " [105 167]]\n",
      "\n",
      "=== ğŸ§¾ Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.651     0.863     0.742       227\n",
      "           1      0.843     0.614     0.711       272\n",
      "\n",
      "    accuracy                          0.727       499\n",
      "   macro avg      0.747     0.739     0.727       499\n",
      "weighted avg      0.756     0.727     0.725       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. ë²¡í„°í™” (BoW ë°©ì‹, ë‹¨ìˆœ ë‹¨ì–´ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§•)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. ëª¨ë¸ í•™ìŠµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== ğŸ“Š Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== ğŸ§¾ Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜', 'ë¡œ', 'ìœ¼ë¡œ', \n",
    "    'ì™€', 'ê³¼', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ê°™ì´', 'ê¹Œì§€', 'ë§ˆì €', 'ì¡°ì°¨', 'ë¶€í„°', \n",
    "    'ì´ë‚˜', 'ë‚˜', 'ì´ë©°', 'ë©°', 'ë“±', 'í•˜ë‹¤', 'í•œë‹¤', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´', \n",
    "    'ë˜ì–´', 'ë˜ë‹¤', 'ë˜ê³ ', 'ë˜ë‹ˆ', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ã…‚ë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ë‹¤', \n",
    "    'ê³ ', 'ë©´', 'ë©°', 'ê²Œ', 'ì§€', 'ì£ ',\n",
    "    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë”°ë¼ì„œ', \n",
    "    'ë˜í•œ', 'ë˜ëŠ”', 'ë°', 'ì¦‰', 'í•œí¸', 'ë°˜ë©´ì—', 'ê·¼ë°',\n",
    "    'ë‚˜', 'ì €', 'ìš°ë¦¬', 'ì €í¬', 'ë„ˆ', 'ë„ˆí¬', 'ë‹¹ì‹ ', 'ê·¸', 'ê·¸ë…€', 'ê·¸ë“¤', 'ëˆ„êµ¬', \n",
    "    'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ëŠ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', \n",
    "    'ì´ìª½', 'ê·¸ìª½', 'ì €ìª½',\n",
    "    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',\n",
    "    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­', 'ë°±', 'ì²œ', 'ë§Œ',\n",
    "    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸',\n",
    "    'ë°”ë¡œ', 'ë•Œ', 'ê²ƒ', 'ìˆ˜', 'ì¼', 'ë¬¸ì œ', 'ê²½ìš°', 'ë¶€ë¶„', \n",
    "    'ë‚´ìš©', 'ê²°ê³¼', 'ìì²´', 'ê°€ì§€',\n",
    "    'ë¿', 'ëŒ€ë¡œ', 'ë§Œí¼', 'ë§Œ', 'ì§€', 'ë”°ë¦„', 'ë‚˜ë¦„', 'ê¹€ì—', 'í„°', 'ë„ˆë¬´', 'ì–´ìš”'\n",
    "    'ì•„', 'ì•„ì´ê³ ', 'ì•„ì´êµ¬', 'ì•„í•˜', 'ì–´', 'ê·¸ë˜', 'ì‘', 'ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ca024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TF-IDF ë²¡í„°í™”\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== TF-IDF Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== TF-IDF Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ebefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1~2ê·¸ë¨ ì ìš© (ex: \"ë§›ìˆì–´ìš”\", \"ì •ë§ ë§›ìˆì–´ìš”\")\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ngram_vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test_ng = ngram_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_ng, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred_ng = model.predict(X_test_ng)\n",
    "\n",
    "print(\"=== N-gram Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_ng))\n",
    "print(\"\\n=== N-gram Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_ng, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + N-gram (1~2ê·¸ë¨)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d265904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_df = pd.read_csv(\"ratings_train_extended.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test_extended.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 2. TF-IDF ë²¡í„°í™” + N-gram(1,2)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# 5. ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_  # shape: (3, n_features)\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ\n",
    "topn = 10\n",
    "weights = {}\n",
    "for idx, class_label in enumerate(model.classes_):  # -1, 0, 1\n",
    "    top_indices = np.argsort(coef[idx])[::-1][:topn]\n",
    "    weights[class_label] = pd.DataFrame({\n",
    "        'word': feature_names[top_indices],\n",
    "        'weight': coef[idx][top_indices]\n",
    "    })\n",
    "\n",
    "# 6. ì‹œê°í™” ì¤€ë¹„ìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_plot = pd.concat(\n",
    "    [df.assign(label=str(label)) for label, df in weights.items()],\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# 7. ì‹œê°í™”: í´ë˜ìŠ¤ë³„ ë‹¨ì–´ ì¤‘ìš”ë„\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Macì´ë¼ë©´ AppleGothic, Windowsë©´ Malgun Gothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "labels = ['-1 (ë¶€ì •)', '0 (ì¤‘ë¦½)', '1 (ê¸ì •)']\n",
    "color_dict = {'-1': '#e74c3c', '0': '#f1c40f', '1': '#2ecc71'}\n",
    "\n",
    "for i, label in enumerate(['-1', '0', '1']):\n",
    "    subset = df_plot[df_plot['label'] == label]\n",
    "    sns.barplot(\n",
    "        ax=axes[i],\n",
    "        data=subset,\n",
    "        y='word',\n",
    "        x='weight',\n",
    "        color=color_dict[label]\n",
    "    )\n",
    "    axes[i].set_title(f\"ê°ì„± {labels[i]} ìƒìœ„ ë‹¨ì–´\")\n",
    "    axes[i].set_xlabel(\"ê°€ì¤‘ì¹˜(weight)\")\n",
    "    axes[i].set_ylabel(\"ë‹¨ì–´\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# ì›Œë“œí´ë¼ìš°ë“œìš© ë‹¨ì–´ + ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸°\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "\n",
    "# ì›Œë“œí´ë¼ìš°ë“œ ê·¸ë¦¬ê¸° í•¨ìˆ˜\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ ì›Œë“œí´ë¼ìš°ë“œ ì¶œë ¥\n",
    "draw_wordcloud(word_weights[-1], 'ë¶€ì • ê°ì„± ì£¼ìš” ë‹¨ì–´', 'Reds')\n",
    "draw_wordcloud(word_weights[0], 'ì¤‘ë¦½ ê°ì„± ì£¼ìš” ë‹¨ì–´', 'Oranges')\n",
    "draw_wordcloud(word_weights[1], 'ê¸ì • ê°ì„± ì£¼ìš” ë‹¨ì–´', 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "new_texts = [\n",
    "    \"ì¹¨ëŒ€ê°€ ë„ˆë¬´ ë¶ˆí¸í•´ì„œ í—ˆë¦¬ê°€ ì•„íŒ ì–´ìš”.\",\n",
    "    \"ì¹œì ˆí•˜ê³  ì¡°ìš©í•´ì„œ í¸í•˜ê²Œ ì‰¬ì—ˆì–´ìš”.\",\n",
    "    \"ê·¸ëƒ¥ ë¬´ë‚œí•œ ìˆ™ì†Œì˜€ì–´ìš”. íŠ¹ë³„í•œ ê±´ ì—†ì—ˆì–´ìš”.\"\n",
    "]\n",
    "\n",
    "# ë²¡í„°í™” (í•™ìŠµí•œ vectorizer ì‚¬ìš©)\n",
    "X_new = vectorizer.transform(new_texts)\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"ë¬¸ì¥: {text}\")\n",
    "    print(f\"ì˜ˆì¸¡ ê°ì„±: {predictions[i]} (í™•ë¥ : {probs[i]})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
