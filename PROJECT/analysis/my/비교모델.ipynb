{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04787b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "\n",
    "stopwords = set(w.lower() for w in stopwords)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return [\n",
    "            word for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 데이터 로딩 및 정제\n",
    "train_df = pd.read_csv(\"36000_reviews_label.csv\", encoding=\"utf-8-sig\")\n",
    "train_df = train_df[train_df['label'].isin([0, 1])]\n",
    "X_train = train_df[\"sentence\"]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "# 모델 A: CountVectorizer + LogisticRegression\n",
    "vectorizer_a = CountVectorizer()\n",
    "X_train_a = vectorizer_a.fit_transform(X_train)\n",
    "model_a = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model_a.fit(X_train_a, y_train)\n",
    "\n",
    "# 모델 B: TF-IDF(ngram) + LogisticRegression\n",
    "vectorizer_b = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_b = vectorizer_b.fit_transform(X_train)\n",
    "model_b = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model_b.fit(X_train_b, y_train)\n",
    "\n",
    "# 저장\n",
    "# joblib.dump(model_a, 'model_logistic_count.pkl')\n",
    "# joblib.dump(vectorizer_a, 'vectorizer_count.pkl')\n",
    "\n",
    "# joblib.dump(model_b, 'model_logistic_tfidf.pkl')\n",
    "# joblib.dump(vectorizer_b, 'vectorizer_tfidf.pkl')\n",
    "\n",
    "print(\"✅ 두 모델 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d10f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"36000_reviews.csv\", encoding=\"utf-8-sig\")\n",
    "texts = df['sentence'].fillna('').astype(str)\n",
    "\n",
    "model_a = joblib.load(\"models/model_logistic_count.pkl\")\n",
    "vectorizer_a = joblib.load(\"models/vectorizer_count.pkl\")\n",
    "\n",
    "X_new_a = vectorizer_a.transform(texts)\n",
    "df['pred_count'] = model_a.predict(X_new_a)\n",
    "df['prob_count'] = model_a.predict_proba(X_new_a)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d35d4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = joblib.load(\"models/model_logistic_tfidf.pkl\")\n",
    "vectorizer_b = joblib.load(\"models/vectorizer_tfidf.pkl\")\n",
    "\n",
    "X_new_b = vectorizer_b.transform(texts)\n",
    "df['예측 클래스'] = model_b.predict(X_new_b)\n",
    "df['긍정 확률'] = model_b.predict_proba(X_new_b)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf6ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_예측'] = df['예측 클래스'].map({0: '부정', 1: '긍정'})\n",
    "df['tfidf_예측'] = df['긍정 확률'].map({0: '부정', 1: '긍정'})\n",
    "\n",
    "df.to_csv(\"숙소_리뷰_감성_분석_결과.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 파일 로드\n",
    "file_path = \"숙소_리뷰_감성_분석_결과.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 기본 컬럼 이름 정리\n",
    "df.columns = df.columns.str.strip()\n",
    "df.rename(columns={\n",
    "    'name': 'accommodation',\n",
    "    'sentence': 'review',\n",
    "    'sentiment_tfidf': 'sentiment'  # 이 라인!\n",
    "\n",
    "}, inplace=True)\n",
    "\n",
    "# 필수 컬럼만 추출\n",
    "df = df[['accommodation', 'review', 'sentiment']]\n",
    "\n",
    "# 1. 숙소별 감성 통계 요약\n",
    "summary = df.groupby(['accommodation', 'sentiment'])['review'].count().unstack(fill_value=0)\n",
    "summary['total'] = summary.sum(axis=1)\n",
    "\n",
    "# 감성 비율 계산\n",
    "for label in ['긍정', '부정', '중립']:\n",
    "    if label in summary.columns:\n",
    "        summary[f'{label}_ratio'] = (summary[label] / summary['total'] * 100).round(1)\n",
    "    else:\n",
    "        summary[label] = 0\n",
    "        summary[f'{label}_ratio'] = 0.0\n",
    "\n",
    "# 2. 감성별 예시 문장 추출 함수\n",
    "def extract_examples(df, label, n=2):\n",
    "    return df[df['sentiment'] == label].groupby('accommodation')['review'].apply(lambda x: x.head(n).tolist())\n",
    "\n",
    "# 예시 문장 추가\n",
    "summary['긍정_예시'] = extract_examples(df, '긍정')\n",
    "summary['부정_예시'] = extract_examples(df, '부정')\n",
    "summary['중립_예시'] = extract_examples(df, '중립')\n",
    "\n",
    "# 결측치 보완\n",
    "summary.fillna({'긍정_예시': '', '부정_예시': '', '중립_예시': ''}, inplace=True)\n",
    "\n",
    "# 필요하면 저장\n",
    "summary.to_csv(\"숙소별_감성분석_요약.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# 일부 미리보기\n",
    "print(summary[['긍정', '부정', '중립', '긍정_ratio', '부정_ratio', '중립_ratio', '긍정_예시', '부정_예시']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 기반 + 키워드 기반 중립 보정 로직을 적용\n",
    "df = pd.read_csv('숙소_리뷰_감성_분석_결과.csv', encoding='utf-8-sig')\n",
    "# 중립 판단 기준 키워드 리스트\n",
    "neutral_clues = ['무난', '그냥', '나쁘지', '괜찮긴', '보통', '가격 대비', '애매', '글쎄', '평범', '그럭저럭']\n",
    "\n",
    "# 숫자 예측값을 텍스트 라벨로 변환\n",
    "def convert_label(pred):\n",
    "    if pred == 1 or pred == '1':\n",
    "        return '긍정'\n",
    "    elif pred == 0 or pred == '0':\n",
    "        return '부정'\n",
    "    return str(pred)\n",
    "\n",
    "df['pred_tfidf'] = df['pred_tfidf'].apply(convert_label)\n",
    "\n",
    "# 중립 보정 함수\n",
    "def smart_reclassify(row, threshold_diff=0.1, prob_threshold=0.6):\n",
    "    prob = row['prob_tfidf']\n",
    "    pred = row['pred_tfidf']\n",
    "    oppo_prob = 1 - prob\n",
    "    diff = abs(prob - oppo_prob)\n",
    "\n",
    "    prob_based = (diff < threshold_diff) or (max(prob, oppo_prob) < prob_threshold)\n",
    "    review = str(row['sentence'])\n",
    "    keyword_based = any(kw in review for kw in neutral_clues)\n",
    "\n",
    "    if prob_based or keyword_based:\n",
    "        return '중립'\n",
    "    return pred\n",
    "\n",
    "# 새 컬럼 추가\n",
    "df['smart_reclassified'] = df.apply(smart_reclassify, axis=1)\n",
    "\n",
    "# 저장\n",
    "df.to_csv('중립_보정_결과.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ab25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('중립_보정_결과.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('중립_보정_결과.csv', encoding='utf-8-sig')\n",
    "df[['sentence', 'smart_reclassified']].to_csv('중립_보정_결과_내용만.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
