{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    'Ïù¥', 'Í∞Ä', 'ÏùÄ', 'Îäî', 'ÏùÑ', 'Î•º', 'Ïùò', 'Ïóê', 'ÏóêÏÑú', 'ÏóêÍ≤å', 'Íªò', 'Î°ú', 'ÏúºÎ°ú', \n",
    "    'ÏôÄ', 'Í≥º', 'Î≥¥Îã§', 'Ï≤òÎüº', 'ÎßåÌÅº', 'Í∞ôÏù¥', 'ÍπåÏßÄ', 'ÎßàÏ†Ä', 'Ï°∞Ï∞®', 'Î∂ÄÌÑ∞', \n",
    "    'Ïù¥ÎÇò', 'ÎÇò', 'Ïù¥Î©∞', 'Î©∞', 'Îì±', 'ÌïòÎã§', 'ÌïúÎã§', 'ÌïòÍ≥†', 'ÌïòÎãà', 'ÌïòÎ©¥', \n",
    "    'ÎêòÏñ¥', 'ÎêòÎã§', 'ÎêòÍ≥†', 'ÎêòÎãà', 'ÏûÖÎãàÎã§', 'ÏäµÎãàÎã§', '„ÖÇÎãàÎã§', 'Ïñ¥Ïöî', 'ÏïÑÏöî', 'Îã§', 'Î∞©Ïù¥', 'Ï†úÎåÄÎ°ú',\n",
    "    'Í≥†', 'Î©¥', 'Í≤å', 'ÏßÄ', 'Ï£†',\n",
    "    'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü¨ÎÇò', 'ÌïòÏßÄÎßå', 'Í∑∏Îü∞Îç∞', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü¨Î©¥', 'Í∑∏Îü¨ÎØÄÎ°ú', 'Îî∞ÎùºÏÑú', \n",
    "    'ÎòêÌïú', 'ÎòêÎäî', 'Î∞è', 'Ï¶â', 'ÌïúÌé∏', 'Î∞òÎ©¥Ïóê', 'Í∑ºÎç∞',\n",
    "    'ÎÇò', 'Ï†Ä', 'Ïö∞Î¶¨', 'Ï†ÄÌù¨', 'ÎÑà', 'ÎÑàÌù¨', 'ÎãπÏã†', 'Í∑∏', 'Í∑∏ÎÖÄ', 'Í∑∏Îì§', 'ÎàÑÍµ¨', 'Í∑∏Î†áÎã§',\n",
    "    'Î¨¥Ïóá', 'Ïñ¥Îîî', 'Ïñ∏Ï†ú', 'Ïñ¥Îäê', 'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞', \n",
    "    'Ïù¥Ï™Ω', 'Í∑∏Ï™Ω', 'Ï†ÄÏ™Ω',\n",
    "    'ÌïòÎÇò', 'Îëò', 'ÏÖã', 'ÎÑ∑', 'Îã§ÏÑØ', 'Ïó¨ÏÑØ', 'ÏùºÍ≥±', 'Ïó¨Îçü', 'ÏïÑÌôâ', 'Ïó¥',\n",
    "    'Ïùº', 'Ïù¥', 'ÏÇº', 'ÏÇ¨', 'Ïò§', 'Ïú°', 'Ïπ†', 'Ìåî', 'Íµ¨', 'Ïã≠', 'Î∞±', 'Ï≤ú', 'Îßå',\n",
    "    'Ï≤´Ïß∏', 'ÎëòÏß∏', 'ÏÖãÏß∏',\n",
    "    'Î∞îÎ°ú', 'Îïå', 'Í≤É', 'Ïàò', 'Î¨∏Ï†ú', 'Í≤ΩÏö∞', 'Î∂ÄÎ∂Ñ', 'Ïù¥Îã§',\n",
    "    'ÎÇ¥Ïö©', 'Í≤∞Í≥º', 'ÏûêÏ≤¥', 'Í∞ÄÏßÄ', 'ÏûàÎã§',\n",
    "    'ÏïäÏïòÏñ¥Ïöî', 'ÏûàÏóàÏñ¥Ïöî', 'ÌñàÏñ¥Ïöî', 'ÌñàÎäîÎç∞Ïöî', 'ÏûàÎäîÎç∞Ïöî', 'Ìï©ÎãàÎã§', 'ÏóÜÎã§', 'ÎÇòÎã§','ÏÉùÍ∞ÅÌïòÎã§',\n",
    "    'ÌñàÎã§', 'Í∞ôÎã§', 'ÎÑ§Ïöî','ÏïÑÎãàÎã§',\n",
    "    'Ï¢Ä', 'ÎÑàÎ¨¥', 'Ï†ïÎßê', 'ÎßéÏù¥', 'Ï°∞Í∏à',\n",
    "    'ÏÇ¨Ïû•', 'Ïù¥Ïö©', 'Ïö©ÌïòÎã§', 'Î¨ºÏù¥',\n",
    "    'Îøê', 'ÎåÄÎ°ú', 'Îßå', 'Îî∞Î¶Ñ', 'ÎÇòÎ¶Ñ', 'ÍπÄÏóê', 'ÌÑ∞',\n",
    "    'ÏïÑ', 'ÏïÑÏù¥Í≥†', 'ÏïÑÏù¥Íµ¨', 'ÏïÑÌïò', 'Ïñ¥', 'Í∑∏Îûò', 'Ïùë', 'ÎÑ§', 'Ïòà', 'ÏïÑÎãà', 'ÏïäÎã§', 'ÏïàÎêòÎã§','Ïïà','Í∑∏ÎÉ•',\n",
    "    'Í∞ÄÎã§', 'Ïò§Îã§', 'Ï£ºÎã§', 'ÎßêÎã§', 'ÎÇòÎã§', 'Î∞õÎã§', 'ÏïåÎã§', 'Î™®Î•¥Îã§', 'Ïã∂Îã§', 'ÏÉùÍ∞ÅÌïòÎã§', 'Îì§Îã§'\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96428718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[172  42]\n",
      " [ 56 231]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.754     0.804     0.778       214\n",
      "           1      0.846     0.805     0.825       287\n",
      "\n",
      "    accuracy                          0.804       501\n",
      "   macro avg      0.800     0.804     0.802       501\n",
      "weighted avg      0.807     0.804     0.805       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ (Ï§ëÎ¶Ω Ï†úÍ±∞ Ìè¨Ìï®)\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 2. ‚úÖ ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ + tokenizer Ìï®Ïàò Ï†ïÏùò\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "\n",
    "# 3. ‚úÖ TF-IDF Î≤°ÌÑ∞Ìôî with tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# 4. Î™®Îç∏ ÌïôÏäµ\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. ÌèâÍ∞Ä\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180f73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Î™®Îç∏Í≥º Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû• ÏôÑÎ£å!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Î™®Îç∏Í≥º Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû•\n",
    "joblib.dump(model, 'models/logistic_model.pkl')\n",
    "joblib.dump(vectorizer, 'models/logistic_tfdf_vectorizer.pkl')\n",
    "\n",
    "print(\"‚úÖ Î™®Îç∏Í≥º Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû• ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Îã®Ïñ¥ Î¶¨Ïä§Ìä∏ Î∞è Í∞ÄÏ§ëÏπò\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]  # Ïù¥ÏßÑ Î∂ÑÎ•òÏù¥ÎØÄÎ°ú shape (1, n_features)\n",
    "\n",
    "# 2. Í∏çÏ†ï/Î∂ÄÏ†ï top Îã®Ïñ¥ Ïù∏Îç±Ïä§ Ï∂îÏ∂ú\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "# 3. Í∏çÏ†ï / Î∂ÄÏ†ï Îã®Ïñ¥Î≥Ñ Í∞ÄÏ§ëÏπò ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
    "word_weights = {\n",
    "    1: dict(zip(feature_names[top_pos_idx], coef[top_pos_idx])),\n",
    "    0: dict(zip(feature_names[top_neg_idx], coef[top_neg_idx])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8625098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(word_weight_dict, title, color='Greens'):\n",
    "    wc = WordCloud(\n",
    "        font_path='C:/Windows/Fonts/NanumGothic.ttf',  # MacÏùÄ AppleGothic, LinuxÎäî ÎÇòÎàîÌè∞Ìä∏\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94260d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(word_weights[0], 'neg', color='Reds')\n",
    "draw_wordcloud(word_weights[1], 'pos', color='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label ÏòàÏ∏°_ÎùºÎ≤®  ÌôïÎ•†Í∏∞Î∞ò_ÏòàÏ∏°  \\\n",
      "0  ÏßÅÏõê ÏπúÏ†àÌïòÍ≥† Ïù¥Ïö©Ïóê Î∂àÌé∏Ìï®ÏùÄ ÏóÜÏúºÎÇò ÏãúÌä∏ Ïò§Ïóº Î∞îÎã•Ï≤≠ÏÜå Î®∏Î¶¨Ïπ¥ÎùΩ Ïù¥Î¨ºÏßà ÏöïÏã§ Î¨ºÍ≥†...      0    Î∂ÄÏ†ï        0   \n",
      "1                       Ï¢ãÏïÑÏöî Ï£ºÎ≥Ä Î®πÏùÑ Í≥≥ÎèÑ ÎßéÍ≥† Í∑ºÎç∞ Î∞©Ïù¥ ÎÑàÎ¨¥ ÏûëÏïÑÏöî      1    Í∏çÏ†ï        0   \n",
      "2  ÎÑàÎ¨¥ ÏùºÏ∞ç Í∞ÄÏÑú Í¥úÏ∞ÆÏùÄÏßÄ Ï†ÑÌôîÎ°ú Ïó∞ÎùΩÎìúÎ†∏ÎäîÎç∞ ÏòàÏïΩÌïú Î∞©ÏùÄ Ï≤≠ÏÜåÌï¥Ïïº Ìï¥ÏÑú Ïïà ÎêòÏßÄÎßå ...      1    Í∏çÏ†ï        1   \n",
      "3  Ï≤òÏùå Í∞ÄÎäî Í≥≥Ïù∏Îç∞ Ï≤´ Î∞©Î¨∏Ïù¥ ÎßåÏ°± ÎßåÏ°± ÎåÄÎßåÏ°±ÏûÖÎãàÎã§ Ïù¥Ï†† Ïó¨Í∏∞Î°ú Ï†ïÏ∞©Ìï¥Ïïº Îê† Í±∞ Í∞ô...      1    Í∏çÏ†ï        1   \n",
      "4                                       Ï≤≠Í≤∞ÏÉÅÌÉú ÍµøÏù¥Í≥†Ïöî ÍπîÎÅî      0    Î∂ÄÏ†ï        0   \n",
      "\n",
      "       prob  \n",
      "0  0.304244  \n",
      "1  0.600172  \n",
      "2  0.708067  \n",
      "3  0.749454  \n",
      "4  0.433499  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('balanced_train_final.csv')\n",
    "sentences = df['sentence'].fillna('').astype(str)\n",
    "X_input = vectorizer.transform(sentences)\n",
    "pred = model.predict(X_input)\n",
    "proba = model.predict_proba(X_input)\n",
    "\n",
    "# üîπ ÏòàÏ∏°\n",
    "df['pred'] = model.predict(X_input)\n",
    "df['prob'] = model.predict_proba(X_input)[:, 1]\n",
    "df['ÏòàÏ∏°_ÎùºÎ≤®'] = df['pred'].map({0: 'Î∂ÄÏ†ï', 1: 'Í∏çÏ†ï'})\n",
    "df['ÌôïÎ•†Í∏∞Î∞ò_ÏòàÏ∏°'] = df['prob'].apply(lambda x: 1 if x >= 0.7 else 0)\n",
    "\n",
    "# üîπ Í≤∞Í≥º ÌôïÏù∏\n",
    "print(df[['sentence', 'label', 'ÏòàÏ∏°_ÎùºÎ≤®', 'ÌôïÎ•†Í∏∞Î∞ò_ÏòàÏ∏°', 'prob']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "570e479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('balance_train_ÏòàÏ∏°.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Ï†ÄÏû•\n",
    "# joblib.dump(model, 'Logistic_model.pkl')\n",
    "# joblib.dump(vectorizer, 'Logistic_tfidf_vectorizer.pkl')\n",
    "# print(\"‚úÖ Î™®Îç∏ Î∞è Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû• ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# ÏõåÎìúÌÅ¥ÎùºÏö∞ÎìúÏö© Îã®Ïñ¥ + Í∞ÄÏ§ëÏπò ÎîïÏÖîÎÑàÎ¶¨ ÎßåÎì§Í∏∞\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "print(word_weights.keys())\n",
    "\n",
    "# ÏõåÎìúÌÅ¥ÎùºÏö∞Îìú Í∑∏Î¶¨Í∏∞ Ìï®Ïàò\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§Î≥Ñ ÏõåÎìúÌÅ¥ÎùºÏö∞Îìú Ï∂úÎ†•\n",
    "draw_wordcloud(word_weights[0], 'Î∂ÄÏ†ï Í∞êÏÑ± Ï£ºÏöî Îã®Ïñ¥', 'Reds')\n",
    "draw_wordcloud(word_weights[1], 'Í∏çÏ†ï Í∞êÏÑ± Ï£ºÏöî Îã®Ïñ¥', 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÉà Î¨∏Ïû• Î¶¨Ïä§Ìä∏\n",
    "df = pd.read_csv('36000_reviews.csv')\n",
    "\n",
    "sentence = df['sentence']\n",
    "\n",
    "# Î≤°ÌÑ∞Ìôî (ÌïôÏäµÌïú vectorizer ÏÇ¨Ïö©)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# ÏòàÏ∏° ÏàòÌñâ\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "threshold = 0.6\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold:\n",
    "        label = \"Í∏çÏ†ï\"\n",
    "    elif prob_pos <= 1 - threshold:\n",
    "        label = \"Î∂ÄÏ†ï\"\n",
    "    else:\n",
    "        label = \"Ï§ëÎ¶Ω\"\n",
    "    \n",
    "    print(f\"Î¨∏Ïû•: {text}\")\n",
    "    print(f\"ÏòàÏ∏° Í∞êÏÑ±: {label} (Í∏çÏ†ï ÌôïÎ•†: {prob_pos:.3f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
