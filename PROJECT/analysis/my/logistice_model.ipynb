{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96428718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[172  42]\n",
      " [ 56 231]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.754     0.804     0.778       214\n",
      "           1      0.846     0.805     0.825       287\n",
      "\n",
      "    accuracy                          0.804       501\n",
      "   macro avg      0.800     0.804     0.802       501\n",
      "weighted avg      0.807     0.804     0.805       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. 데이터 불러오기 (중립 제거 포함)\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 2. ✅ 형태소 분석기 + tokenizer 함수 정의\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "\n",
    "# 3. ✅ TF-IDF 벡터화 with tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# 4. 모델 학습\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180f73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델과 벡터라이저 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델과 벡터라이저 저장\n",
    "joblib.dump(model, 'models/logistic_model.pkl')\n",
    "joblib.dump(vectorizer, 'models/logistic_tfdf_vectorizer.pkl')\n",
    "\n",
    "print(\"✅ 모델과 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. 단어 리스트 및 가중치\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]  # 이진 분류이므로 shape (1, n_features)\n",
    "\n",
    "# 2. 긍정/부정 top 단어 인덱스 추출\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "# 3. 긍정 / 부정 단어별 가중치 딕셔너리 생성\n",
    "word_weights = {\n",
    "    1: dict(zip(feature_names[top_pos_idx], coef[top_pos_idx])),\n",
    "    0: dict(zip(feature_names[top_neg_idx], coef[top_neg_idx])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8625098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(word_weight_dict, title, color='Greens'):\n",
    "    wc = WordCloud(\n",
    "        font_path='C:/Windows/Fonts/NanumGothic.ttf',  # Mac은 AppleGothic, Linux는 나눔폰트\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94260d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(word_weights[0], 'neg', color='Reds')\n",
    "draw_wordcloud(word_weights[1], 'pos', color='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 잠은 잘 잤는데 냄새났어요\n",
      "예측: 0, 확률: [0.8672514 0.1327486]\n",
      "문장: 서비스는 좋은데 시설이 별로였어요\n",
      "예측: 1, 확률: [0.48667371 0.51332629]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\"잠은 잘 잤는데 냄새났어요\", \"서비스는 좋은데 시설이 별로였어요\"]\n",
    "X_new = vectorizer.transform(new_texts)\n",
    "pred = model.predict(X_new)\n",
    "proba = model.predict_proba(X_new)\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측: {pred[i]}, 확률: {proba[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 저장\n",
    "# joblib.dump(model, 'Logistic_model.pkl')\n",
    "# joblib.dump(vectorizer, 'Logistic_tfidf_vectorizer.pkl')\n",
    "# print(\"✅ 모델 및 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드용 단어 + 가중치 딕셔너리 만들기\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "print(word_weights.keys())\n",
    "\n",
    "# 워드클라우드 그리기 함수\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# 클래스별 워드클라우드 출력\n",
    "draw_wordcloud(word_weights[0], '부정 감성 주요 단어', 'Reds')\n",
    "draw_wordcloud(word_weights[1], '긍정 감성 주요 단어', 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 문장 리스트\n",
    "df = pd.read_csv('36000_reviews.csv')\n",
    "\n",
    "sentence = df['sentence']\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "threshold = 0.6\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold:\n",
    "        label = \"긍정\"\n",
    "    elif prob_pos <= 1 - threshold:\n",
    "        label = \"부정\"\n",
    "    else:\n",
    "        label = \"중립\"\n",
    "    \n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측 감성: {label} (긍정 확률: {prob_pos:.3f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
