{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "\n",
    "stopwords = set(w.lower() for w in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d78f5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 📊 Confusion Matrix ===\n",
      "[[ 49  42  30]\n",
      " [ 26  53  64]\n",
      " [  5  18 214]]\n",
      "\n",
      "=== 🧾 Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.613     0.405     0.488       121\n",
      "           0      0.469     0.371     0.414       143\n",
      "           1      0.695     0.903     0.785       237\n",
      "\n",
      "    accuracy                          0.631       501\n",
      "   macro avg      0.592     0.560     0.562       501\n",
      "weighted avg      0.610     0.631     0.607       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. CSV 파일 불러오기\n",
    "train_df = pd.read_csv(\"updated_ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"updated_ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df['label'].isin([-1, 0, 1])]\n",
    "test_df = test_df[test_df['label'].isin([-1, 0, 1])]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "# 2. 벡터화 (BoW 방식, 단순 단어 단위 토크나이징)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== 📊 Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== 🧾 Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델과 벡터라이저 저장\n",
    "joblib.dump(model, 'models/sentiment_model.pkl')\n",
    "joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"✅ 모델과 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f91c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 그냥 그럼\n",
      "예측 감성: 0 (긍정 확률: 0.872)\n",
      "\n",
      "문장: 진짜 별로에요\n",
      "예측 감성: 1 (긍정 확률: 0.294)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"그냥 그럼\", \"진짜 별로에요\"]\n",
    "X_new = vectorizer.transform(new_sentences)\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "for i, text in enumerate(new_sentences):\n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측 감성: {predictions[i]} (긍정 확률: {probs[i][1]:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF Confusion Matrix ===\n",
      "[[ 38  13  70]\n",
      " [ 15  24 104]\n",
      " [  3   3 231]]\n",
      "\n",
      "=== TF-IDF Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.679     0.314     0.429       121\n",
      "           0      0.600     0.168     0.262       143\n",
      "           1      0.570     0.975     0.720       237\n",
      "\n",
      "    accuracy                          0.585       501\n",
      "   macro avg      0.616     0.486     0.470       501\n",
      "weighted avg      0.605     0.585     0.519       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== TF-IDF Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== TF-IDF Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960ebefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-gram Confusion Matrix ===\n",
      "[[ 47  43  31]\n",
      " [ 28  52  63]\n",
      " [  5  19 213]]\n",
      "\n",
      "=== N-gram Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.588     0.388     0.468       121\n",
      "           0      0.456     0.364     0.405       143\n",
      "           1      0.694     0.899     0.783       237\n",
      "\n",
      "    accuracy                          0.623       501\n",
      "   macro avg      0.579     0.550     0.552       501\n",
      "weighted avg      0.600     0.623     0.599       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1~2그램 적용 (ex: \"맛있어요\", \"정말 맛있어요\")\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ngram_vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test_ng = ngram_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# 모델 학습\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_ng, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred_ng = model.predict(X_test_ng)\n",
    "\n",
    "print(\"=== N-gram Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_ng))\n",
    "print(\"\\n=== N-gram Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_ng, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + N-gram (1~2그램)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9282cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40  23  58]\n",
      " [ 18  39  86]\n",
      " [  6   6 225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.625     0.331     0.432       121\n",
      "           0      0.574     0.273     0.370       143\n",
      "           1      0.610     0.949     0.743       237\n",
      "\n",
      "    accuracy                          0.607       501\n",
      "   macro avg      0.603     0.518     0.515       501\n",
      "weighted avg      0.603     0.607     0.561       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "\n",
    "# 1. 데이터 불러오기\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ✅ 2. 긍정(1)과 부정(0)만 남기고, 그 외 중립 제거\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])].copy()\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])].copy()\n",
    "okt = Okt()\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return [word for word, pos in okt.pos(text, stem=True) \n",
    "                if pos in ['Noun', 'Adjective'] \n",
    "                and word not in stopwords\n",
    "                and len(word) > 1\n",
    "                ]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# 3. 벡터화\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 4. 모델 학습 (클래스 가중치 balanced)\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "\n",
    "# 6. 중요 단어 추출 (이진 분류는 coef_[0] 사용)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "# 7. 시각화\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Mac이면 AppleGothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"부정 상위 단어 (label=0)\")\n",
    "axes[0].set_xlabel(\"가중치(weight)\")\n",
    "axes[0].set_ylabel(\"단어\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"긍정 상위 단어 (label=1)\")\n",
    "axes[1].set_xlabel(\"가중치(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 새 문장 리스트\n",
    "df = pd.read_csv('36000_reviews.csv', encoding='utf-8-sig')\n",
    "\n",
    "sentence = df['sentence']\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "labels = []\n",
    "threshold_low = 0.3\n",
    "threshold_high = 0.5\n",
    "\n",
    "labels = []\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold_high:\n",
    "        label = \"긍정\"\n",
    "    elif prob_pos <= threshold_low:\n",
    "        label = \"부정\"\n",
    "    else:\n",
    "        label = \"중립\"\n",
    "    labels.append(label)\n",
    "\n",
    "df['미세조정하면'] = labels\n",
    "\n",
    "df.to_csv('리뷰_라벨.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84112844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "top_pos_idx = [ item for item in top_pos_idx if item not in stopwords ]\n",
    "top_neg_idx = [ item for item in top_neg_idx if item not in stopwords ]\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "# 7. 시각화\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Mac이면 AppleGothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"부정 상위 단어 (label=0)\")\n",
    "axes[0].set_xlabel(\"가중치(weight)\")\n",
    "axes[0].set_ylabel(\"단어\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"긍정 상위 단어 (label=1)\")\n",
    "axes[1].set_xlabel(\"가중치(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f93086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-base-ko-sentence-correction\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"KETI-AIR/ke-t5-base-ko-sentence-correction\")\n",
    "\n",
    "# input_sentence = \"아버지가방에들어가신다\"\n",
    "# input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(input_ids, max_length=128)\n",
    "# corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(\"수정 전:\", input_sentence)\n",
    "# print(\"수정 후:\", corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "# client = InferenceClient(\n",
    "#     provider=\"featherless-ai\",\n",
    "#     api_key=os.environ[\"HF_TOKEN\"],\n",
    "# )\n",
    "\n",
    "# result = client.text_generation(\n",
    "#     \"Can you please let us know more details about your \",\n",
    "#     model=\"upstage/SOLAR-10.7B-v1.0\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
