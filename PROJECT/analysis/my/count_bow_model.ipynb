{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    'Ïù¥', 'Í∞Ä', 'ÏùÄ', 'Îäî', 'ÏùÑ', 'Î•º', 'Ïùò', 'Ïóê', 'ÏóêÏÑú', 'ÏóêÍ≤å', 'Íªò', 'Î°ú', 'ÏúºÎ°ú', \n",
    "    'ÏôÄ', 'Í≥º', 'Î≥¥Îã§', 'Ï≤òÎüº', 'ÎßåÌÅº', 'Í∞ôÏù¥', 'ÍπåÏßÄ', 'ÎßàÏ†Ä', 'Ï°∞Ï∞®', 'Î∂ÄÌÑ∞', \n",
    "    'Ïù¥ÎÇò', 'ÎÇò', 'Ïù¥Î©∞', 'Î©∞', 'Îì±', 'ÌïòÎã§', 'ÌïúÎã§', 'ÌïòÍ≥†', 'ÌïòÎãà', 'ÌïòÎ©¥', \n",
    "    'ÎêòÏñ¥', 'ÎêòÎã§', 'ÎêòÍ≥†', 'ÎêòÎãà', 'ÏûÖÎãàÎã§', 'ÏäµÎãàÎã§', '„ÖÇÎãàÎã§', 'Ïñ¥Ïöî', 'ÏïÑÏöî', 'Îã§', 'Î∞©Ïù¥', 'Ï†úÎåÄÎ°ú',\n",
    "    'Í≥†', 'Î©¥', 'Í≤å', 'ÏßÄ', 'Ï£†',\n",
    "    'Í∑∏Î¶¨Í≥†', 'Í∑∏Îü¨ÎÇò', 'ÌïòÏßÄÎßå', 'Í∑∏Îü∞Îç∞', 'Í∑∏ÎûòÏÑú', 'Í∑∏Îü¨Î©¥', 'Í∑∏Îü¨ÎØÄÎ°ú', 'Îî∞ÎùºÏÑú', \n",
    "    'ÎòêÌïú', 'ÎòêÎäî', 'Î∞è', 'Ï¶â', 'ÌïúÌé∏', 'Î∞òÎ©¥Ïóê', 'Í∑ºÎç∞',\n",
    "    'ÎÇò', 'Ï†Ä', 'Ïö∞Î¶¨', 'Ï†ÄÌù¨', 'ÎÑà', 'ÎÑàÌù¨', 'ÎãπÏã†', 'Í∑∏', 'Í∑∏ÎÖÄ', 'Í∑∏Îì§', 'ÎàÑÍµ¨', 'Í∑∏Î†áÎã§',\n",
    "    'Î¨¥Ïóá', 'Ïñ¥Îîî', 'Ïñ∏Ï†ú', 'Ïñ¥Îäê', 'Ïù¥Í≤É', 'Í∑∏Í≤É', 'Ï†ÄÍ≤É', 'Ïó¨Í∏∞', 'Í±∞Í∏∞', 'Ï†ÄÍ∏∞', \n",
    "    'Ïù¥Ï™Ω', 'Í∑∏Ï™Ω', 'Ï†ÄÏ™Ω',\n",
    "    'ÌïòÎÇò', 'Îëò', 'ÏÖã', 'ÎÑ∑', 'Îã§ÏÑØ', 'Ïó¨ÏÑØ', 'ÏùºÍ≥±', 'Ïó¨Îçü', 'ÏïÑÌôâ', 'Ïó¥',\n",
    "    'Ïùº', 'Ïù¥', 'ÏÇº', 'ÏÇ¨', 'Ïò§', 'Ïú°', 'Ïπ†', 'Ìåî', 'Íµ¨', 'Ïã≠', 'Î∞±', 'Ï≤ú', 'Îßå',\n",
    "    'Ï≤´Ïß∏', 'ÎëòÏß∏', 'ÏÖãÏß∏',\n",
    "    'Î∞îÎ°ú', 'Îïå', 'Í≤É', 'Ïàò', 'Î¨∏Ï†ú', 'Í≤ΩÏö∞', 'Î∂ÄÎ∂Ñ', 'Ïù¥Îã§',\n",
    "    'ÎÇ¥Ïö©', 'Í≤∞Í≥º', 'ÏûêÏ≤¥', 'Í∞ÄÏßÄ',\n",
    "    'ÏïäÏïòÏñ¥Ïöî', 'ÏûàÏóàÏñ¥Ïöî', 'ÌñàÏñ¥Ïöî', 'ÌñàÎäîÎç∞Ïöî', 'ÏûàÎäîÎç∞Ïöî', 'Ìï©ÎãàÎã§', 'ÏóÜÎã§', 'ÎÇòÎã§','ÏÉùÍ∞ÅÌïòÎã§',\n",
    "    'ÌñàÎã§', 'Í∞ôÎã§', 'ÎÑ§Ïöî','ÏïÑÎãàÎã§',\n",
    "    'Ï¢Ä', 'ÎÑàÎ¨¥', 'Ï†ïÎßê', 'ÎßéÏù¥', 'Ï°∞Í∏à',\n",
    "    'ÏÇ¨Ïû•', 'Ïù¥Ïö©', 'Ïö©ÌïòÎã§', 'Î¨ºÏù¥',\n",
    "    'Îøê', 'ÎåÄÎ°ú', 'Îßå', 'Îî∞Î¶Ñ', 'ÎÇòÎ¶Ñ', 'ÍπÄÏóê', 'ÌÑ∞',\n",
    "    'ÏïÑ', 'ÏïÑÏù¥Í≥†', 'ÏïÑÏù¥Íµ¨', 'ÏïÑÌïò', 'Ïñ¥', 'Í∑∏Îûò', 'Ïùë', 'ÎÑ§', 'Ïòà', 'ÏïÑÎãà', 'ÏïäÎã§', 'ÏïàÎêòÎã§','Ïïà','Í∑∏ÎÉ•',\n",
    "    'Í∞ÄÎã§', 'Ïò§Îã§', 'Ï£ºÎã§', 'ÎßêÎã§', 'ÎÇòÎã§', 'Î∞õÎã§', 'ÏïåÎã§', 'Î™®Î•¥Îã§', 'Ïã∂Îã§', 'ÏÉùÍ∞ÅÌïòÎã§', 'Îì§Îã§'\n",
    "]))\n",
    "\n",
    "stopwords = set(w.lower() for w in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d78f5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== üìä Confusion Matrix ===\n",
      "[[ 49  42  30]\n",
      " [ 26  53  64]\n",
      " [  5  18 214]]\n",
      "\n",
      "=== üßæ Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.613     0.405     0.488       121\n",
      "           0      0.469     0.371     0.414       143\n",
      "           1      0.695     0.903     0.785       237\n",
      "\n",
      "    accuracy                          0.631       501\n",
      "   macro avg      0.592     0.560     0.562       501\n",
      "weighted avg      0.610     0.631     0.607       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. CSV ÌååÏùº Î∂àÎü¨Ïò§Í∏∞\n",
    "train_df = pd.read_csv(\"updated_ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"updated_ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df['label'].isin([-1, 0, 1])]\n",
    "test_df = test_df[test_df['label'].isin([-1, 0, 1])]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "\n",
    "# 2. Î≤°ÌÑ∞Ìôî (BoW Î∞©Ïãù, Îã®Ïàú Îã®Ïñ¥ Îã®ÏúÑ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3. Î™®Îç∏ ÌïôÏäµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== üìä Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== üßæ Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af5d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Î™®Îç∏Í≥º Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû•\n",
    "joblib.dump(model, 'models/sentiment_model.pkl')\n",
    "joblib.dump(vectorizer, 'models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"‚úÖ Î™®Îç∏Í≥º Î≤°ÌÑ∞ÎùºÏù¥Ï†Ä Ï†ÄÏû• ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f91c680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Î¨∏Ïû•: Í∑∏ÎÉ• Í∑∏Îüº\n",
      "ÏòàÏ∏° Í∞êÏÑ±: 0 (Í∏çÏ†ï ÌôïÎ•†: 0.872)\n",
      "\n",
      "Î¨∏Ïû•: ÏßÑÏßú Î≥ÑÎ°úÏóêÏöî\n",
      "ÏòàÏ∏° Í∞êÏÑ±: 1 (Í∏çÏ†ï ÌôïÎ•†: 0.294)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_sentences = [\"Í∑∏ÎÉ• Í∑∏Îüº\", \"ÏßÑÏßú Î≥ÑÎ°úÏóêÏöî\"]\n",
    "X_new = vectorizer.transform(new_sentences)\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "for i, text in enumerate(new_sentences):\n",
    "    print(f\"Î¨∏Ïû•: {text}\")\n",
    "    print(f\"ÏòàÏ∏° Í∞êÏÑ±: {predictions[i]} (Í∏çÏ†ï ÌôïÎ•†: {probs[i][1]:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714ca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TF-IDF Confusion Matrix ===\n",
      "[[ 38  13  70]\n",
      " [ 15  24 104]\n",
      " [  3   3 231]]\n",
      "\n",
      "=== TF-IDF Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.679     0.314     0.429       121\n",
      "           0      0.600     0.168     0.262       143\n",
      "           1      0.570     0.975     0.720       237\n",
      "\n",
      "    accuracy                          0.585       501\n",
      "   macro avg      0.616     0.486     0.470       501\n",
      "weighted avg      0.605     0.585     0.519       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# TF-IDF Î≤°ÌÑ∞Ìôî\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(train_df[\"text\"])\n",
    "X_test = tfidf.transform(test_df[\"text\"])\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== TF-IDF Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== TF-IDF Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "960ebefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-gram Confusion Matrix ===\n",
      "[[ 47  43  31]\n",
      " [ 28  52  63]\n",
      " [  5  19 213]]\n",
      "\n",
      "=== N-gram Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.588     0.388     0.468       121\n",
      "           0      0.456     0.364     0.405       143\n",
      "           1      0.694     0.899     0.783       237\n",
      "\n",
      "    accuracy                          0.623       501\n",
      "   macro avg      0.579     0.550     0.552       501\n",
      "weighted avg      0.600     0.623     0.599       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1~2Í∑∏Îû® Ï†ÅÏö© (ex: \"ÎßõÏûàÏñ¥Ïöî\", \"Ï†ïÎßê ÎßõÏûàÏñ¥Ïöî\")\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_ng = ngram_vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test_ng = ngram_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_ng, y_train)\n",
    "\n",
    "# ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "y_pred_ng = model.predict(X_test_ng)\n",
    "\n",
    "print(\"=== N-gram Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_ng))\n",
    "print(\"\\n=== N-gram Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_ng, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + N-gram (1~2Í∑∏Îû®)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9282cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40  23  58]\n",
      " [ 18  39  86]\n",
      " [  6   6 225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.625     0.331     0.432       121\n",
      "           0      0.574     0.273     0.370       143\n",
      "           1      0.610     0.949     0.743       237\n",
      "\n",
      "    accuracy                          0.607       501\n",
      "   macro avg      0.603     0.518     0.515       501\n",
      "weighted avg      0.603     0.607     0.561       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ‚úÖ 2. Í∏çÏ†ï(1)Í≥º Î∂ÄÏ†ï(0)Îßå ÎÇ®Í∏∞Í≥†, Í∑∏ Ïô∏ Ï§ëÎ¶Ω Ï†úÍ±∞\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])].copy()\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])].copy()\n",
    "okt = Okt()\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return [word for word, pos in okt.pos(text, stem=True) \n",
    "                if pos in ['Noun', 'Adjective'] \n",
    "                and word not in stopwords\n",
    "                and len(word) > 1\n",
    "                ]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# 3. Î≤°ÌÑ∞Ìôî\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 4. Î™®Îç∏ ÌïôÏäµ (ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò balanced)\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=2))\n",
    "\n",
    "# 6. Ï§ëÏöî Îã®Ïñ¥ Ï∂îÏ∂ú (Ïù¥ÏßÑ Î∂ÑÎ•òÎäî coef_[0] ÏÇ¨Ïö©)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "# 7. ÏãúÍ∞ÅÌôî\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # MacÏù¥Î©¥ AppleGothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"Î∂ÄÏ†ï ÏÉÅÏúÑ Îã®Ïñ¥ (label=0)\")\n",
    "axes[0].set_xlabel(\"Í∞ÄÏ§ëÏπò(weight)\")\n",
    "axes[0].set_ylabel(\"Îã®Ïñ¥\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"Í∏çÏ†ï ÏÉÅÏúÑ Îã®Ïñ¥ (label=1)\")\n",
    "axes[1].set_xlabel(\"Í∞ÄÏ§ëÏπò(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ÏÉà Î¨∏Ïû• Î¶¨Ïä§Ìä∏\n",
    "df = pd.read_csv('36000_reviews.csv', encoding='utf-8-sig')\n",
    "\n",
    "sentence = df['sentence']\n",
    "\n",
    "# Î≤°ÌÑ∞Ìôî (ÌïôÏäµÌïú vectorizer ÏÇ¨Ïö©)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# ÏòàÏ∏° ÏàòÌñâ\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "labels = []\n",
    "threshold_low = 0.3\n",
    "threshold_high = 0.5\n",
    "\n",
    "labels = []\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold_high:\n",
    "        label = \"Í∏çÏ†ï\"\n",
    "    elif prob_pos <= threshold_low:\n",
    "        label = \"Î∂ÄÏ†ï\"\n",
    "    else:\n",
    "        label = \"Ï§ëÎ¶Ω\"\n",
    "    labels.append(label)\n",
    "\n",
    "df['ÎØ∏ÏÑ∏Ï°∞Ï†ïÌïòÎ©¥'] = labels\n",
    "\n",
    "df.to_csv('Î¶¨Î∑∞_ÎùºÎ≤®.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84112844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "top_pos_idx = [ item for item in top_pos_idx if item not in stopwords ]\n",
    "top_neg_idx = [ item for item in top_neg_idx if item not in stopwords ]\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "# 7. ÏãúÍ∞ÅÌôî\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # MacÏù¥Î©¥ AppleGothic\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"Î∂ÄÏ†ï ÏÉÅÏúÑ Îã®Ïñ¥ (label=0)\")\n",
    "axes[0].set_xlabel(\"Í∞ÄÏ§ëÏπò(weight)\")\n",
    "axes[0].set_ylabel(\"Îã®Ïñ¥\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"Í∏çÏ†ï ÏÉÅÏúÑ Îã®Ïñ¥ (label=1)\")\n",
    "axes[1].set_xlabel(\"Í∞ÄÏ§ëÏπò(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f93086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"KETI-AIR/ke-t5-base-ko-sentence-correction\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"KETI-AIR/ke-t5-base-ko-sentence-correction\")\n",
    "\n",
    "# input_sentence = \"ÏïÑÎ≤ÑÏßÄÍ∞ÄÎ∞©ÏóêÎì§Ïñ¥Í∞ÄÏã†Îã§\"\n",
    "# input_ids = tokenizer.encode(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "# outputs = model.generate(input_ids, max_length=128)\n",
    "# corrected = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(\"ÏàòÏ†ï Ï†Ñ:\", input_sentence)\n",
    "# print(\"ÏàòÏ†ï ÌõÑ:\", corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "# client = InferenceClient(\n",
    "#     provider=\"featherless-ai\",\n",
    "#     api_key=os.environ[\"HF_TOKEN\"],\n",
    "# )\n",
    "\n",
    "# result = client.text_generation(\n",
    "#     \"Can you please let us know more details about your \",\n",
    "#     model=\"upstage/SOLAR-10.7B-v1.0\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
