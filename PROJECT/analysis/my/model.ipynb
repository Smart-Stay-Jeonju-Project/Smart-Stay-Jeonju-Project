{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7737604",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "\n",
    "#stopwords = set(w.lower() for w in stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a432825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ [모델 평가 결과]\n",
      "=== Confusion Matrix ===\n",
      "[[171  43]\n",
      " [ 57 230]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.799     0.774       214\n",
      "           1      0.842     0.801     0.821       287\n",
      "\n",
      "    accuracy                          0.800       501\n",
      "   macro avg      0.796     0.800     0.798       501\n",
      "weighted avg      0.803     0.800     0.801       501\n",
      "\n",
      "\n",
      "✅ 모델과 벡터라이저가 [models/logistic_regression/v1]에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ====== 설정 ======\n",
    "MODEL_NAME = \"logistic_regression\"\n",
    "VERSION = \"v1\"  # 변경 가능: v1, v2, v20250712 등\n",
    "BASE_DIR = f\"models/{MODEL_NAME}/{VERSION}\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "#stopwords = []  # 필요시 stopwords 채워 넣기\n",
    "\n",
    "# ====== 데이터 로딩 ======\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# 중립 제거\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# ====== 토크나이저 정의 ======\n",
    "okt = Okt()\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "# ====== 벡터화 ======\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# ====== 모델 학습 ======\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ====== 평가 ======\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\n✅ [모델 평가 결과]\")\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ====== 저장 ======\n",
    "joblib.dump(model, os.path.join(BASE_DIR, \"model.pkl\"))\n",
    "joblib.dump(vectorizer, os.path.join(BASE_DIR, \"vectorizer.pkl\"))\n",
    "with open(os.path.join(BASE_DIR, \"stopwords.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stopwords, f, ensure_ascii=False)\n",
    "\n",
    "# 메타 정보 저장\n",
    "metadata = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"description\": \"TF-IDF + Okt 기반 이진 감성 분석\",\n",
    "    \"created_at\": str(datetime.now()),\n",
    "    \"vectorizer\": \"TfidfVectorizer(ngram_range=(1,2))\",\n",
    "    \"stopwords_count\": len(stopwords),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df)\n",
    "}\n",
    "with open(os.path.join(BASE_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"saved_at.txt\"), \"w\") as f:\n",
    "    f.write(str(datetime.now()))\n",
    "\n",
    "print(f\"\\n✅ 모델과 벡터라이저가 [{BASE_DIR}]에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a697aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 📊 Confusion Matrix ===\n",
      "[[110  24  32]\n",
      " [ 29  23  20]\n",
      " [ 34  17 212]]\n",
      "\n",
      "=== 🧾 Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.636     0.663     0.649       166\n",
      "           0      0.359     0.319     0.338        72\n",
      "           1      0.803     0.806     0.805       263\n",
      "\n",
      "    accuracy                          0.689       501\n",
      "   macro avg      0.599     0.596     0.597       501\n",
      "weighted avg      0.684     0.689     0.686       501\n",
      "\n",
      "\n",
      "✅ 모델이 저장되었습니다: saved_model\\logistic_model.pkl\n",
      "✅ 벡터라이저 저장됨: saved_model\\tfidf_vectorizer.pkl\n",
      "\n",
      "🧪 예시 문장 테스트:\n",
      "문장: 위치는 괜찮았지만 다시 가고 싶진 않네요.\n",
      "예측 감성: 중립\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from konlpy.tag import Okt\n",
    "from functools import partial\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ===== 설정 =====\n",
    "MODEL_DIR = \"saved_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"logistic_model.pkl\")\n",
    "VECTORIZER_PATH = os.path.join(MODEL_DIR, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# ===== 데이터 불러오기 =====\n",
    "train_df = pd.read_csv(\"updated_ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"updated_ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([-1, 0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([-1, 0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# ===== 형태소 분석기 기반 토크나이저 =====\n",
    "okt = Okt()\n",
    "# stopwords = [\"그냥\", \"좀\", \"정도\", \"이런\", \"저런\"]\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in [\"Noun\", \"Adjective\"] and word not in stopwords and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenize Error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "# ===== TF-IDF 벡터화 =====\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# ===== 로지스틱 회귀 모델 학습 =====\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ===== 평가 출력 =====\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== 📊 Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== 🧾 Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ===== 모델 저장 =====\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "print(f\"\\n✅ 모델이 저장되었습니다: {MODEL_PATH}\")\n",
    "print(f\"✅ 벡터라이저 저장됨: {VECTORIZER_PATH}\")\n",
    "\n",
    "# ===== 예측 함수 정의 =====\n",
    "def predict_sentiment(text):\n",
    "    loaded_model = joblib.load(MODEL_PATH)\n",
    "    loaded_vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    loaded_vectorizer.tokenizer = tokenizer  # tokenizer 다시 지정\n",
    "\n",
    "    X_input = loaded_vectorizer.transform([text])\n",
    "    pred = loaded_model.predict(X_input)[0]\n",
    "    \n",
    "    label_map = {-1: \"부정\", 0: \"중립\", 1: \"긍정\"}\n",
    "    return label_map.get(pred, \"알 수 없음\")\n",
    "\n",
    "# ===== 예측 테스트 예시 =====\n",
    "print(\"\\n🧪 예시 문장 테스트:\")\n",
    "example = \"위치는 괜찮았지만 다시 가고 싶진 않네요.\"\n",
    "print(f\"문장: {example}\")\n",
    "print(f\"예측 감성: {predict_sentiment(example)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
