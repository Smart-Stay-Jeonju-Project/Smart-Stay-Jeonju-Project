{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7737604",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜', 'ë¡œ', 'ìœ¼ë¡œ', \n",
    "    'ì™€', 'ê³¼', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ê°™ì´', 'ê¹Œì§€', 'ë§ˆì €', 'ì¡°ì°¨', 'ë¶€í„°', \n",
    "    'ì´ë‚˜', 'ë‚˜', 'ì´ë©°', 'ë©°', 'ë“±', 'í•˜ë‹¤', 'í•œë‹¤', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´', \n",
    "    'ë˜ì–´', 'ë˜ë‹¤', 'ë˜ê³ ', 'ë˜ë‹ˆ', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ã…‚ë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ë‹¤', 'ë°©ì´', 'ì œëŒ€ë¡œ',\n",
    "    'ê³ ', 'ë©´', 'ê²Œ', 'ì§€', 'ì£ ',\n",
    "    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë”°ë¼ì„œ', \n",
    "    'ë˜í•œ', 'ë˜ëŠ”', 'ë°', 'ì¦‰', 'í•œí¸', 'ë°˜ë©´ì—', 'ê·¼ë°',\n",
    "    'ë‚˜', 'ì €', 'ìš°ë¦¬', 'ì €í¬', 'ë„ˆ', 'ë„ˆí¬', 'ë‹¹ì‹ ', 'ê·¸', 'ê·¸ë…€', 'ê·¸ë“¤', 'ëˆ„êµ¬', 'ê·¸ë ‡ë‹¤',\n",
    "    'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ëŠ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', \n",
    "    'ì´ìª½', 'ê·¸ìª½', 'ì €ìª½',\n",
    "    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',\n",
    "    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­', 'ë°±', 'ì²œ', 'ë§Œ',\n",
    "    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸',\n",
    "    'ë°”ë¡œ', 'ë•Œ', 'ê²ƒ', 'ìˆ˜', 'ë¬¸ì œ', 'ê²½ìš°', 'ë¶€ë¶„', 'ì´ë‹¤',\n",
    "    'ë‚´ìš©', 'ê²°ê³¼', 'ìì²´', 'ê°€ì§€', 'ìˆë‹¤',\n",
    "    'ì•Šì•˜ì–´ìš”', 'ìˆì—ˆì–´ìš”', 'í–ˆì–´ìš”', 'í–ˆëŠ”ë°ìš”', 'ìˆëŠ”ë°ìš”', 'í•©ë‹ˆë‹¤', 'ì—†ë‹¤', 'ë‚˜ë‹¤','ìƒê°í•˜ë‹¤',\n",
    "    'í–ˆë‹¤', 'ê°™ë‹¤', 'ë„¤ìš”','ì•„ë‹ˆë‹¤',\n",
    "    'ì¢€', 'ë„ˆë¬´', 'ì •ë§', 'ë§ì´', 'ì¡°ê¸ˆ',\n",
    "    'ì‚¬ì¥', 'ì´ìš©', 'ìš©í•˜ë‹¤', 'ë¬¼ì´',\n",
    "    'ë¿', 'ëŒ€ë¡œ', 'ë§Œ', 'ë”°ë¦„', 'ë‚˜ë¦„', 'ê¹€ì—', 'í„°',\n",
    "    'ì•„', 'ì•„ì´ê³ ', 'ì•„ì´êµ¬', 'ì•„í•˜', 'ì–´', 'ê·¸ë˜', 'ì‘', 'ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆ', 'ì•Šë‹¤', 'ì•ˆë˜ë‹¤','ì•ˆ','ê·¸ëƒ¥',\n",
    "    'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ì£¼ë‹¤', 'ë§ë‹¤', 'ë‚˜ë‹¤', 'ë°›ë‹¤', 'ì•Œë‹¤', 'ëª¨ë¥´ë‹¤', 'ì‹¶ë‹¤', 'ìƒê°í•˜ë‹¤', 'ë“¤ë‹¤'\n",
    "]))\n",
    "\n",
    "#stopwords = set(w.lower() for w in stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a432825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… [ëª¨ë¸ í‰ê°€ ê²°ê³¼]\n",
      "=== Confusion Matrix ===\n",
      "[[171  43]\n",
      " [ 57 230]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.750     0.799     0.774       214\n",
      "           1      0.842     0.801     0.821       287\n",
      "\n",
      "    accuracy                          0.800       501\n",
      "   macro avg      0.796     0.800     0.798       501\n",
      "weighted avg      0.803     0.800     0.801       501\n",
      "\n",
      "\n",
      "âœ… ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì €ê°€ [models/logistic_regression/v1]ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ====== ì„¤ì • ======\n",
    "MODEL_NAME = \"logistic_regression\"\n",
    "VERSION = \"v1\"  # ë³€ê²½ ê°€ëŠ¥: v1, v2, v20250712 ë“±\n",
    "BASE_DIR = f\"models/{MODEL_NAME}/{VERSION}\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "#stopwords = []  # í•„ìš”ì‹œ stopwords ì±„ì›Œ ë„£ê¸°\n",
    "\n",
    "# ====== ë°ì´í„° ë¡œë”© ======\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# ì¤‘ë¦½ ì œê±°\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# ====== í† í¬ë‚˜ì´ì € ì •ì˜ ======\n",
    "okt = Okt()\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "# ====== ë²¡í„°í™” ======\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# ====== ëª¨ë¸ í•™ìŠµ ======\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ====== í‰ê°€ ======\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nâœ… [ëª¨ë¸ í‰ê°€ ê²°ê³¼]\")\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ====== ì €ì¥ ======\n",
    "joblib.dump(model, os.path.join(BASE_DIR, \"model.pkl\"))\n",
    "joblib.dump(vectorizer, os.path.join(BASE_DIR, \"vectorizer.pkl\"))\n",
    "with open(os.path.join(BASE_DIR, \"stopwords.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stopwords, f, ensure_ascii=False)\n",
    "\n",
    "# ë©”íƒ€ ì •ë³´ ì €ì¥\n",
    "metadata = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"description\": \"TF-IDF + Okt ê¸°ë°˜ ì´ì§„ ê°ì„± ë¶„ì„\",\n",
    "    \"created_at\": str(datetime.now()),\n",
    "    \"vectorizer\": \"TfidfVectorizer(ngram_range=(1,2))\",\n",
    "    \"stopwords_count\": len(stopwords),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"test_size\": len(test_df)\n",
    "}\n",
    "with open(os.path.join(BASE_DIR, \"metadata.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"saved_at.txt\"), \"w\") as f:\n",
    "    f.write(str(datetime.now()))\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì €ê°€ [{BASE_DIR}]ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a697aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ğŸ“Š Confusion Matrix ===\n",
      "[[110  24  32]\n",
      " [ 29  23  20]\n",
      " [ 34  17 212]]\n",
      "\n",
      "=== ğŸ§¾ Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      0.636     0.663     0.649       166\n",
      "           0      0.359     0.319     0.338        72\n",
      "           1      0.803     0.806     0.805       263\n",
      "\n",
      "    accuracy                          0.689       501\n",
      "   macro avg      0.599     0.596     0.597       501\n",
      "weighted avg      0.684     0.689     0.686       501\n",
      "\n",
      "\n",
      "âœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: saved_model\\logistic_model.pkl\n",
      "âœ… ë²¡í„°ë¼ì´ì € ì €ì¥ë¨: saved_model\\tfidf_vectorizer.pkl\n",
      "\n",
      "ğŸ§ª ì˜ˆì‹œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸:\n",
      "ë¬¸ì¥: ìœ„ì¹˜ëŠ” ê´œì°®ì•˜ì§€ë§Œ ë‹¤ì‹œ ê°€ê³  ì‹¶ì§„ ì•Šë„¤ìš”.\n",
      "ì˜ˆì¸¡ ê°ì„±: ì¤‘ë¦½\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from konlpy.tag import Okt\n",
    "from functools import partial\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "MODEL_DIR = \"saved_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"logistic_model.pkl\")\n",
    "VECTORIZER_PATH = os.path.join(MODEL_DIR, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# ===== ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° =====\n",
    "train_df = pd.read_csv(\"updated_ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"updated_ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([-1, 0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([-1, 0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# ===== í˜•íƒœì†Œ ë¶„ì„ê¸° ê¸°ë°˜ í† í¬ë‚˜ì´ì € =====\n",
    "okt = Okt()\n",
    "# stopwords = [\"ê·¸ëƒ¥\", \"ì¢€\", \"ì •ë„\", \"ì´ëŸ°\", \"ì €ëŸ°\"]\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in [\"Noun\", \"Adjective\"] and word not in stopwords and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenize Error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "# ===== TF-IDF ë²¡í„°í™” =====\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# ===== ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ =====\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ===== í‰ê°€ ì¶œë ¥ =====\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== ğŸ“Š Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n=== ğŸ§¾ Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ===== ëª¨ë¸ ì €ì¥ =====\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "joblib.dump(vectorizer, VECTORIZER_PATH)\n",
    "print(f\"\\nâœ… ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {MODEL_PATH}\")\n",
    "print(f\"âœ… ë²¡í„°ë¼ì´ì € ì €ì¥ë¨: {VECTORIZER_PATH}\")\n",
    "\n",
    "# ===== ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ =====\n",
    "def predict_sentiment(text):\n",
    "    loaded_model = joblib.load(MODEL_PATH)\n",
    "    loaded_vectorizer = joblib.load(VECTORIZER_PATH)\n",
    "    loaded_vectorizer.tokenizer = tokenizer  # tokenizer ë‹¤ì‹œ ì§€ì •\n",
    "\n",
    "    X_input = loaded_vectorizer.transform([text])\n",
    "    pred = loaded_model.predict(X_input)[0]\n",
    "    \n",
    "    label_map = {-1: \"ë¶€ì •\", 0: \"ì¤‘ë¦½\", 1: \"ê¸ì •\"}\n",
    "    return label_map.get(pred, \"ì•Œ ìˆ˜ ì—†ìŒ\")\n",
    "\n",
    "# ===== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ =====\n",
    "print(\"\\nğŸ§ª ì˜ˆì‹œ ë¬¸ì¥ í…ŒìŠ¤íŠ¸:\")\n",
    "example = \"ìœ„ì¹˜ëŠ” ê´œì°®ì•˜ì§€ë§Œ ë‹¤ì‹œ ê°€ê³  ì‹¶ì§„ ì•Šë„¤ìš”.\"\n",
    "print(f\"ë¬¸ì¥: {example}\")\n",
    "print(f\"ì˜ˆì¸¡ ê°ì„±: {predict_sentiment(example)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
