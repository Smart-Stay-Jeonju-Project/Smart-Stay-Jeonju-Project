{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04787b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = list(set([\n",
    "    # 조사, 접속사\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '부터', '이나', '나', '이며', '며', \n",
    "    '등', '고', '면', '게', '지', '죠',\n",
    "    # 불필요한 구조적 단어\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '따라서', '또한', '즉',\n",
    "    # 불분명한 대명사\n",
    "    '나', '저', '우리', '너', '당신', '그', '그녀', '그것', '이것', '저것', \n",
    "    # 수사 및 조사적 명사\n",
    "    '하나', '둘', '셋', '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백',\n",
    "    # 너무 범용적인 단어\n",
    "    '것', '수', '문제', '내용', '경우', '정도',\n",
    "    # 감정과 무관한 구어 표현\n",
    "    '아', '어', '응', '음', '네', '예', '그래',\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3ab640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        return [\n",
    "            word for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective', 'Verb']\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tokenize() got an unexpected keyword argument 'stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m     10\u001b[0m texts_tokenized \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokenize(text)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m---> 11\u001b[0m X_input \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_tokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 예측\u001b[39;00m\n\u001b[0;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_input)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: tokenize() got an unexpected keyword argument 'stopwords'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(\"logistic_model.pkl\")\n",
    "vectorizer = joblib.load(\"logistic_tfdf_vectorizer.pkl\")\n",
    "\n",
    "df = pd.read_csv(\"balanced_test_final.csv\", encoding=\"utf-8-sig\")\n",
    "texts = df['sentence'].fillna('').astype(str)\n",
    "\n",
    "texts_tokenized = [' '.join(tokenize(text)) for text in texts]\n",
    "X_input = vectorizer.transform(texts_tokenized)\n",
    "\n",
    "# 예측\n",
    "df['pred'] = model.predict(X_input)\n",
    "df['prob'] = model.predict_proba(X_input)[:, 1]\n",
    "df['예측_라벨'] = df['pred'].map({0: '부정', 1: '긍정'})\n",
    "df['확률기반_예측'] = df['prob'].apply(lambda x: '긍정' if x >= 0.5 else '부정')\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv(\"예측결과_서비스용.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 데이터 로딩 및 정제\n",
    "train_df = pd.read_csv(\"36000_reviews_label.csv\", encoding=\"utf-8-sig\")\n",
    "train_df = train_df[train_df['label'].isin([0, 1])]\n",
    "X_train = train_df[\"sentence\"]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "# 모델 A: CountVectorizer + LogisticRegression\n",
    "vectorizer_a = CountVectorizer()\n",
    "X_train_a = vectorizer_a.fit_transform(X_train)\n",
    "model_a = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model_a.fit(X_train_a, y_train)\n",
    "\n",
    "# 모델 B: TF-IDF(ngram) + LogisticRegression\n",
    "vectorizer_b = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_b = vectorizer_b.fit_transform(X_train)\n",
    "model_b = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model_b.fit(X_train_b, y_train)\n",
    "\n",
    "# 저장\n",
    "# joblib.dump(model_a, 'model_logistic_count.pkl')\n",
    "# joblib.dump(vectorizer_a, 'vectorizer_count.pkl')\n",
    "\n",
    "# joblib.dump(model_b, 'model_logistic_tfidf.pkl')\n",
    "# joblib.dump(vectorizer_b, 'vectorizer_tfidf.pkl')\n",
    "\n",
    "print(\"✅ 두 모델 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d35d4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = joblib.load(\"models/model_logistic_tfidf.pkl\")\n",
    "vectorizer_b = joblib.load(\"models/vectorizer_tfidf.pkl\")\n",
    "\n",
    "X_new_b = vectorizer_b.transform(texts)\n",
    "df['예측 클래스'] = model_b.predict(X_new_b)\n",
    "df['긍정 확률'] = model_b.predict_proba(X_new_b)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf6ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"테스트_숙소_리뷰_감성_분석_결과.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 파일 로드\n",
    "file_path = \"숙소_리뷰_감성_분석_결과.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 기본 컬럼 이름 정리\n",
    "df.columns = df.columns.str.strip()\n",
    "df.rename(columns={\n",
    "    'name': 'accommodation',\n",
    "    'sentence': 'review',\n",
    "    'sentiment_tfidf': 'sentiment'  # 이 라인!\n",
    "\n",
    "}, inplace=True)\n",
    "\n",
    "# 필수 컬럼만 추출\n",
    "df = df[['accommodation', 'review', 'sentiment']]\n",
    "\n",
    "# 1. 숙소별 감성 통계 요약\n",
    "summary = df.groupby(['accommodation', 'sentiment'])['review'].count().unstack(fill_value=0)\n",
    "summary['total'] = summary.sum(axis=1)\n",
    "\n",
    "# 감성 비율 계산\n",
    "for label in ['긍정', '부정', '중립']:\n",
    "    if label in summary.columns:\n",
    "        summary[f'{label}_ratio'] = (summary[label] / summary['total'] * 100).round(1)\n",
    "    else:\n",
    "        summary[label] = 0\n",
    "        summary[f'{label}_ratio'] = 0.0\n",
    "\n",
    "# 2. 감성별 예시 문장 추출 함수\n",
    "def extract_examples(df, label, n=2):\n",
    "    return df[df['sentiment'] == label].groupby('accommodation')['review'].apply(lambda x: x.head(n).tolist())\n",
    "\n",
    "# 예시 문장 추가\n",
    "summary['긍정_예시'] = extract_examples(df, '긍정')\n",
    "summary['부정_예시'] = extract_examples(df, '부정')\n",
    "summary['중립_예시'] = extract_examples(df, '중립')\n",
    "\n",
    "# 결측치 보완\n",
    "summary.fillna({'긍정_예시': '', '부정_예시': '', '중립_예시': ''}, inplace=True)\n",
    "\n",
    "# 필요하면 저장\n",
    "summary.to_csv(\"숙소별_감성분석_요약.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# 일부 미리보기\n",
    "print(summary[['긍정', '부정', '중립', '긍정_ratio', '부정_ratio', '중립_ratio', '긍정_예시', '부정_예시']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 기반 + 키워드 기반 중립 보정 로직을 적용\n",
    "df = pd.read_csv('숙소_리뷰_감성_분석_결과.csv', encoding='utf-8-sig')\n",
    "# 중립 판단 기준 키워드 리스트\n",
    "neutral_clues = ['무난', '그냥', '나쁘지', '괜찮긴', '보통', '가격 대비', '애매', '글쎄', '평범', '그럭저럭']\n",
    "\n",
    "# 숫자 예측값을 텍스트 라벨로 변환\n",
    "def convert_label(pred):\n",
    "    if pred == 1 or pred == '1':\n",
    "        return '긍정'\n",
    "    elif pred == 0 or pred == '0':\n",
    "        return '부정'\n",
    "    return str(pred)\n",
    "\n",
    "df['pred_tfidf'] = df['pred_tfidf'].apply(convert_label)\n",
    "\n",
    "# 중립 보정 함수\n",
    "def smart_reclassify(row, threshold_diff=0.1, prob_threshold=0.6):\n",
    "    prob = row['prob_tfidf']\n",
    "    pred = row['pred_tfidf']\n",
    "    oppo_prob = 1 - prob\n",
    "    diff = abs(prob - oppo_prob)\n",
    "\n",
    "    prob_based = (diff < threshold_diff) or (max(prob, oppo_prob) < prob_threshold)\n",
    "    review = str(row['sentence'])\n",
    "    keyword_based = any(kw in review for kw in neutral_clues)\n",
    "\n",
    "    if prob_based or keyword_based:\n",
    "        return '중립'\n",
    "    return pred\n",
    "\n",
    "# 새 컬럼 추가\n",
    "df['smart_reclassified'] = df.apply(smart_reclassify, axis=1)\n",
    "\n",
    "# 저장\n",
    "df.to_csv('중립_보정_결과.csv', encoding='utf-8-sig', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ab25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('중립_보정_결과.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2d8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('중립_보정_결과.csv', encoding='utf-8-sig')\n",
    "df[['sentence', 'smart_reclassified']].to_csv('중립_보정_결과_내용만.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
