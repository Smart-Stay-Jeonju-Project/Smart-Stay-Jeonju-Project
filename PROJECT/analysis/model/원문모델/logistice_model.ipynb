{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96428718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[172  42]\n",
      " [ 56 231]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.754     0.804     0.778       214\n",
      "           1      0.846     0.805     0.825       287\n",
      "\n",
      "    accuracy                          0.804       501\n",
      "   macro avg      0.800     0.804     0.802       501\n",
      "weighted avg      0.807     0.804     0.805       501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. 데이터 불러오기 (중립 제거 포함)\n",
    "train_df = pd.read_csv(\"ratings_train.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 2. ✅ 형태소 분석기 + tokenizer 함수 정의\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "\n",
    "# 3. ✅ TF-IDF 벡터화 with tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# 4. 모델 학습\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180f73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델과 벡터라이저 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델과 벡터라이저 저장\n",
    "joblib.dump(model, 'logistic_model.pkl')\n",
    "joblib.dump(vectorizer, 'logistic_tfdf_vectorizer.pkl')\n",
    "\n",
    "print(\"✅ 모델과 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U ipywidgets\n",
    "#jupyter nbextension enable --py widgetsnbextension\n",
    "#!pip install --upgrade huggingface_hub\n",
    "#!pip install --upgrade transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb328ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "0.33.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import huggingface_hub\n",
    "\n",
    "print(transformers.__version__)   # ex: 4.42.1\n",
    "print(huggingface_hub.__version__)  # ex: 0.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b790eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ratings_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f4fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.14-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.3-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mycom\\.conda\\envs\\azen\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading aiohttp-3.12.14-cp310-cp310-win_amd64.whl (451 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.3-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "   ------ ---------------------------------  2/13 [multidict]\n",
      "  Attempting uninstall: fsspec\n",
      "   ------ ---------------------------------  2/13 [multidict]\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "   ------ ---------------------------------  2/13 [multidict]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   --------- ------------------------------  3/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [frozenlist]\n",
      "   --------------- ------------------------  5/13 [dill]\n",
      "   --------------- ------------------------  5/13 [dill]\n",
      "   --------------- ------------------------  5/13 [dill]\n",
      "   --------------- ------------------------  5/13 [dill]\n",
      "   --------------- ------------------------  5/13 [dill]\n",
      "   --------------------- ------------------  7/13 [aiohappyeyeballs]\n",
      "   ------------------------ ---------------  8/13 [yarl]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   ------------------------------ --------- 10/13 [aiosignal]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ---------------------------------------- 13/13 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.3 multiprocess-0.70.16 propcache-0.3.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.2 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.17.2 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e98cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 2. 훈련/검증 분할\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# 3. 토크나이저 준비\n",
    "tokenizer = BertTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# 4. HuggingFace Dataset 객체로 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 5. 텐서 변환\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 6. 모델 로딩\n",
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=2)\n",
    "\n",
    "# 7. 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobert_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# 8. 평가 함수 정의\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# 9. Trainer 구성 및 학습\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. 단어 리스트 및 가중치\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]  # 이진 분류이므로 shape (1, n_features)\n",
    "\n",
    "# 2. 긍정/부정 top 단어 인덱스 추출\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "# 3. 긍정 / 부정 단어별 가중치 딕셔너리 생성\n",
    "word_weights = {\n",
    "    1: dict(zip(feature_names[top_pos_idx], coef[top_pos_idx])),\n",
    "    0: dict(zip(feature_names[top_neg_idx], coef[top_neg_idx])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8625098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_wordcloud(word_weight_dict, title, color='Greens'):\n",
    "    wc = WordCloud(\n",
    "        font_path='C:/Windows/Fonts/NanumGothic.ttf',  # Mac은 AppleGothic, Linux는 나눔폰트\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94260d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(word_weights[0], 'neg', color='Reds')\n",
    "draw_wordcloud(word_weights[1], 'pos', color='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 잠은 잘 잤는데 냄새났어요\n",
      "예측: 0, 확률: [0.8672514 0.1327486]\n",
      "문장: 서비스는 좋은데 시설이 별로였어요\n",
      "예측: 1, 확률: [0.48667371 0.51332629]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\"잠은 잘 잤는데 냄새났어요\", \"서비스는 좋은데 시설이 별로였어요\"]\n",
    "X_new = vectorizer.transform(new_texts)\n",
    "pred = model.predict(X_new)\n",
    "proba = model.predict_proba(X_new)\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측: {pred[i]}, 확률: {proba[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 저장\n",
    "# joblib.dump(model, 'Logistic_model.pkl')\n",
    "# joblib.dump(vectorizer, 'Logistic_tfidf_vectorizer.pkl')\n",
    "# print(\"✅ 모델 및 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드용 단어 + 가중치 딕셔너리 만들기\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "print(word_weights.keys())\n",
    "\n",
    "# 워드클라우드 그리기 함수\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# 클래스별 워드클라우드 출력\n",
    "draw_wordcloud(word_weights[0], '부정 감성 주요 단어', 'Reds')\n",
    "draw_wordcloud(word_weights[1], '긍정 감성 주요 단어', 'Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 문장 리스트\n",
    "df = pd.read_csv('36000_reviews.csv')\n",
    "\n",
    "sentence = df['sentence']\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "threshold = 0.6\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold:\n",
    "        label = \"긍정\"\n",
    "    elif prob_pos <= 1 - threshold:\n",
    "        label = \"부정\"\n",
    "    else:\n",
    "        label = \"중립\"\n",
    "    \n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측 감성: {label} (긍정 확률: {prob_pos:.3f})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
