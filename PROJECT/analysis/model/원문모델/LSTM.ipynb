{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c584315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # 한글, 숫자만 남기고 나머지 제거\n",
    "    text = re.sub(r\"[^가-힣0-9\\s]\", \"\", str(text))\n",
    "    words = text.split()\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25aefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "워드 인덱스 단어 개수 : 7609\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/max_word_index.json\",\"r\", encoding='utf-8') as f:\n",
    "    word_index = json.load(f)\n",
    "print(f\"워드 인덱스 단어 개수 : {len(word_index)}\")\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b4cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 하이퍼 파라메타인 임베딩 백터\n",
    "embedding_dim = 64\n",
    "hidden_units = 64\n",
    "# 샘플의 원소 개수 제한\n",
    "max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47fe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력층 설정\n",
    "model.add(Embedding(\n",
    "    # 입력 단어의 종류 수 : 단어 사전의 단어 개수\n",
    "    10000,             # 정수 인덱스의 최대 값\n",
    "    # 각 단어를 몇개의 특성으로 표현할 것인가\n",
    "    embedding_dim,          # 임베딩 백터 차원 값\n",
    "    input_length=max_length # 훈련 데이터의 특성 개수 : 샘플의 원소 개수\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329049c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 학습 데이터 가져오기\n",
    "with open(\"X_train_sequences.pickle\",\"rb\") as fr:\n",
    "    X_train = pickle.load(fr)\n",
    "with open(\"X_test_sequences.pickle\",\"rb\") as fr:\n",
    "    X_test = pickle.load(fr)\n",
    "# 테스트 데이터 가져오기\n",
    "with open(\"y_train_filterd.pickle\",\"rb\") as fr:\n",
    "    y_train = pickle.load(fr)\n",
    "with open(\"y_test_filterd.pickle\",\"rb\") as fr:\n",
    "    y_test = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c06339",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"로드된 데이터 개수 :\")\n",
    "print(f\"X_train : {len(X_train)} / y_train : {len(y_train)}\")\n",
    "print(f\"X_test : {len(X_test)} / y_test : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d593d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 원래 텍스트 (str) 리스트가 필요\n",
    "train_df = pd.read_csv(\"data/정제_train_data.csv\")\n",
    "X_train_raw = train_df['text'].astype(str).apply(clean_text).tolist()\n",
    "\n",
    "# 2. 토크나이저 학습\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_raw)\n",
    "\n",
    "# 3. 시퀀스로 변환\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_raw)\n",
    "\n",
    "# 4. 시퀀스를 패딩\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=70, padding='post', truncating='post', dtype='int32')\n",
    "\n",
    "# 5. 라벨\n",
    "y_train = train_df['label'].astype(np.int32).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^가-힣0-9\\s]\", \"\", str(text))\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# 데이터 로드\n",
    "train_df = pd.read_csv(\"data/정제_train_data.csv\")\n",
    "test_df = pd.read_csv(\"data/정제_test_data.csv\")\n",
    "\n",
    "# 전처리\n",
    "train_df['text'] = train_df['text'].astype(str).apply(clean_text)\n",
    "test_df['text'] = test_df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# 텍스트 리스트\n",
    "X_train = train_df['text'].tolist()\n",
    "X_test = test_df['text'].tolist()\n",
    "\n",
    "# 라벨 리스트\n",
    "y_train = train_df['label'].astype(np.int32).values\n",
    "y_test = test_df['label'].astype(np.int32).values\n",
    "\n",
    "# 4. 토크나이저 학습 및 정수 인코딩\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 5. 길이 제한 후 다시 쌍 정렬\n",
    "def trim_samples(X, y, max_len):\n",
    "    X_out, y_out = [], []\n",
    "    for x, label in zip(X, y):\n",
    "        if len(x) <= max_len:\n",
    "            X_out.append(x)\n",
    "            y_out.append(label)\n",
    "    return X_out, y_out\n",
    "\n",
    "# 시퀀스 변환\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 패딩\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"라벨 {label} 개수: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f13ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 실험 이름 정의 (날짜/시간 + 주요 하이퍼파라미터)\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "experiment_name = f\"exp_{timestamp}_lr{learning_rate}_bs64_lstm64\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "log_dir = f\"./logs/{experiment_name}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# CSVLogger: epoch마다 val_acc, loss 등 기록\n",
    "csv_logger = CSVLogger(os.path.join(log_dir, 'training_log.csv'))\n",
    "\n",
    "# EarlyStopping & ModelCheckpoint (같이 사용)\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "mc = ModelCheckpoint(os.path.join(log_dir, 'best_model.h5'),\n",
    "                     monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=max_len),\n",
    "    LSTM(64),   # 유닛수 증가\n",
    "    Dropout(0.7), # 드롭아웃 증가\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.4), # 드롭아웃 증가\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# 컴파일\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 변경\n",
    "#BATCH_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# 훈련\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=12,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es, mc, csv_logger],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "\n",
    "# history 결과에서 마지막 성능 요약 저장\n",
    "final_log_path = os.path.join(log_dir, 'summary.csv')\n",
    "with open(final_log_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['param', 'value'])\n",
    "    writer.writerow(['timestamp', timestamp])\n",
    "    writer.writerow(['embedding_dim', embedding_dim])\n",
    "    writer.writerow(['lstm_units', hidden_units])\n",
    "    writer.writerow(['learning_rate', learning_rate])\n",
    "    writer.writerow(['batch_size', BATCH_SIZE])\n",
    "    writer.writerow(['max_len', max_len])\n",
    "    writer.writerow(['val_accuracy_last', history.history['val_accuracy'][-1]])\n",
    "    writer.writerow(['val_loss_last', history.history['val_loss'][-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    pad = pad_sequences(seq, maxlen=70, padding='post', truncating='post')\n",
    "    prob = model.predict(pad)[0][0]\n",
    "    label = '긍정' if prob > 0.5 else '부정'\n",
    "    print(f\"[리뷰] {text}\\n→ 예측: {label} (확률: {prob:.2f})\")\n",
    "    return label, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3168a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"사장님이 정말 친절하고 방도 깨끗해요!\")\n",
    "predict_sentiment(\"방음이 너무 안되고 시설이 낡았어요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = os.path.join(log_dir, 'final_model.keras')\n",
    "model.save(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets accelerate\n",
    "#!pip install tf-keras\n",
    "\n",
    "#!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d761b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#  1. 데이터 로딩\n",
    "df = pd.read_csv(\"data/정제_train_data.csv\")  # 반드시 text, label 컬럼 있어야 함\n",
    "df = df[['text', 'label']]  # label: 0=부정, 1=긍정\n",
    "\n",
    "#  2. 학습/검증 분할\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "#  3. Huggingface Datasets 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "#  4. Tokenizer 및 전처리 함수\n",
    "model_name = \"klue/roberta-base\"  # 또는 \"monologg/kobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "#  5. 모델 정의 (2 클래스 분류)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "#  6. 훈련 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True # 추가\n",
    "\n",
    ")\n",
    "\n",
    "#  7. 평가 지표 정의\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = np.mean(preds == labels)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "#  8. Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#  9. 모델 학습\n",
    "trainer.train()\n",
    "\n",
    "#  10. 예측 결과 출력\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "true_labels = preds.label_ids\n",
    "\n",
    "print(\"\\n 평가 결과:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"부정\", \"긍정\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss, acc = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 시각화\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# 예시 출력\n",
    "for i in range(5):\n",
    "    print(f\"{X_test[i]}\")\n",
    "    print(f\"실제: {y_test[i]} / 예측: {y_pred[i][0]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"best_model_keras.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ee8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 예측 확률 → 0.5 기준 이진 분류\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "results_df = pd.DataFrame({\n",
    "    'text': X_test,  # 원문 텍스트\n",
    "    'true_label': y_test,\n",
    "    'pred_label': y_pred,\n",
    "    'pred_prob': y_pred_probs.flatten()\n",
    "})\n",
    "\n",
    "# 저장\n",
    "results_df.to_csv(\"lstm_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"예측 결과가 lstm_predictions.csv에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24942a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe594c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def extract_nouns(texts):\n",
    "    from konlpy.tag import Okt\n",
    "    okt = Okt()\n",
    "    all_nouns = []\n",
    "    for text in texts:\n",
    "        nouns = okt.nouns(str(text))\n",
    "        nouns = [n for n in nouns if len(n) > 1]  # 1글자 제외\n",
    "        all_nouns.extend(nouns)\n",
    "    return all_nouns\n",
    "\n",
    "# 예측 기준 분리\n",
    "positive_texts = results_df[results_df['pred_label'] == 1]['text'].tolist()\n",
    "negative_texts = results_df[results_df['pred_label'] == 0]['text'].tolist()\n",
    "\n",
    "# 키워드 추출\n",
    "positive_nouns = extract_nouns(positive_texts)\n",
    "negative_nouns = extract_nouns(negative_texts)\n",
    "\n",
    "# 빈도수 계산\n",
    "pos_freq = Counter(positive_nouns)\n",
    "neg_freq = Counter(negative_nouns)\n",
    "\n",
    "def generate_wordcloud(freq_dict, title, font_path='NanumGothic.ttf'):\n",
    "    wc = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white'\n",
    "    )\n",
    "    wc_img = wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc_img, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(pos_freq, \"pos\")\n",
    "generate_wordcloud(neg_freq, \"neg\")\n",
    "\n",
    "wc = WordCloud(font_path='NanumGothic.ttf', width=800, height=400, background_color='white')\n",
    "wc.generate_from_frequencies(pos_freq).to_file(\"positive_wordcloud.png\")\n",
    "wc.generate_from_frequencies(neg_freq).to_file(\"negative_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16261b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac108d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fca3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model_keras.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# 모델 로드\n",
    "# model = load_model(\"best_model_keras.keras\")\n",
    "# model = load_model(\"best_model.h5\", compile=False)\n",
    "\n",
    "# 토크나이저 로드\n",
    "import pickle\n",
    "with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# 데이터 불러오기 (예: 실제 리뷰)\n",
    "df = pd.read_csv(\"y_reviews.csv\")\n",
    "texts = df['review_content'].tolist()\n",
    "\n",
    "# 전처리 및 시퀀스 변환\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded = pad_sequences(sequences, maxlen=30)  # maxlen은 학습 때 사용한 값과 동일하게\n",
    "\n",
    "# 예측\n",
    "probs = model.predict(padded)\n",
    "preds = (probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# 결과 추가\n",
    "df['predicted_label'] = preds\n",
    "df['sentiment_score'] = probs.flatten()  # 0~1 감성 점수\n",
    "\n",
    "# 저장\n",
    "df.to_csv(\"예측된_감성_리뷰.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['sentiment_score'], bins=50)\n",
    "plt.title(\"감성 점수 분포\")\n",
    "plt.xlabel(\"sentiment_score\")\n",
    "plt.ylabel(\"리뷰 수\")\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47df2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_3way(score, center=0.5, neutral_margin=0.01):\n",
    "    if abs(score - center) < neutral_margin:\n",
    "        return \"중립\"\n",
    "    elif score >= center + neutral_margin:\n",
    "        return \"긍정\"\n",
    "    else:\n",
    "        return \"부정\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_label_3way'] = df['sentiment_score'].apply(classify_3way)\n",
    "df.to_csv(\"예측된_감성_리뷰.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
