{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', '하다', '있다',\n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다', '방이', '제대로',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '좀', '너무', '정말', '많이', '조금',\n",
    "    '사장', '이용', '용하다', '물이', '매우',\n",
    "    '뿐', '대로', '만', '따름', '나름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebc1f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('accom_review_source.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33630bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_type\n",
       " 1    141108\n",
       "-1     15828\n",
       " 0     14826\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review_type'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['review_type'].unique())\n",
    "print(df['review_type'].map(type).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fe9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from konlpy.tag import Okt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 형태소 분석기 설정\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#print(os.getcwd())  # 현재 작업 디렉토리\n",
    "fm.findSystemFonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from konlpy.tag import Okt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# ----- 한글 폰트 설정 -----\n",
    "font_path = \"C:\\\\Users\\\\MYCOM\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\NotoSansKR-Bold.ttf\"\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=font_prop.get_name())\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# ----- 토크나이저 -----\n",
    "okt = Okt()\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "# ----- 폴더 생성 -----\n",
    "os.makedirs(\"output/wordcloud\", exist_ok=True)\n",
    "os.makedirs(\"output/barplot\", exist_ok=True)\n",
    "\n",
    "# ----- CSV 불러오기 -----\n",
    "df = pd.read_csv('accom_review_type.csv')\n",
    "df['review_type'] = df['review_type'].astype(int)\n",
    "\n",
    "if not {'accommodation_id', 'review_id', 'content', 'review_type'}.issubset(df.columns):\n",
    "    raise ValueError(\"'accommodation_id', 'review_id', 'content', 'review_type' 컬럼이 존재해야 합니다.\")\n",
    "\n",
    "# ----- 분석 시작 -----\n",
    "hotel_names = df['accommodation_id'].unique()\n",
    "\n",
    "for hotel in tqdm(hotel_names, desc=\"숙소별 키워드 분석\"):\n",
    "    df_hotel = df[df['accommodation_id'] == hotel]\n",
    "\n",
    "    for label in [-1, 1]:  # 부정 / 긍정\n",
    "        df_sentiment = df_hotel[df_hotel['review_type'] == label]\n",
    "\n",
    "        if len(df_sentiment) < 5:\n",
    "            continue\n",
    "\n",
    "        # 감정별 추가 불용어 설정\n",
    "        local_stopwords = stopwords.copy()\n",
    "        if label == -1:\n",
    "            local_stopwords += ['좋다', '예쁘다', '깔끔하다', '깨끗하다', '친절하다', '편안하다', '따뜻하다', '만족하다']\n",
    "\n",
    "        # TF-IDF\n",
    "        tfidf = TfidfVectorizer(tokenizer=lambda x: tokenize(x, stopwords=local_stopwords), max_features=1000)\n",
    "        X = tfidf.fit_transform(df_sentiment['content'])\n",
    "        feature_names = np.array(tfidf.get_feature_names_out())\n",
    "        tfidf_mean = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "        # 키워드 정렬 및 필터링\n",
    "        topn = 20\n",
    "        top_idx = np.argsort(tfidf_mean)[::-1][:topn]\n",
    "        top_words = feature_names[top_idx]\n",
    "        top_scores = tfidf_mean[top_idx]\n",
    "\n",
    "        filtered = [(w, s) for w, s in zip(top_words, top_scores) if w not in local_stopwords]\n",
    "\n",
    "        # 워드클라우드용 데이터 (모두 사용)\n",
    "        word_freq = dict(filtered)\n",
    "\n",
    "        # 바 그래프용 상위 20개\n",
    "        bar_keywords = filtered[:20]\n",
    "        bar_words, bar_scores = zip(*bar_keywords) if bar_keywords else ([], [])\n",
    "\n",
    "        # ----- 워드클라우드 저장 -----\n",
    "        suffix = 'pos' if label == 1 else 'neg'\n",
    "        cmap = \"rainbow\" if label == 1 else \"gist_stern\"\n",
    "        wc = WordCloud(\n",
    "            font_path=font_path,\n",
    "            background_color='white',\n",
    "            width=800,\n",
    "            height=400,\n",
    "            colormap=cmap,\n",
    "            max_font_size=100,\n",
    "            relative_scaling=0.3\n",
    "        )\n",
    "        wc.generate_from_frequencies(word_freq)\n",
    "        wc_path = f\"output/wordcloud/{(hotel)}_{suffix}.png\"\n",
    "        wc.to_file(wc_path)\n",
    "\n",
    "        # ----- 바 그래프 저장 -----\n",
    "        if bar_keywords:\n",
    "            df_keywords = pd.DataFrame({'word': bar_words, 'score': bar_scores})\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            base_palette = sns.color_palette(\"Blues\", n_colors=len(df_keywords)) if label == 1 else sns.color_palette(\"Reds\", n_colors=len(df_keywords))\n",
    "            sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
    "            #sns.barplot(data=df_keywords, y='word', x='score', palette='Blues' if label == 1 else 'Reds')\n",
    "            plt.title(f\"{hotel} - {'긍정' if label == 1 else '부정'} 키워드 (상위 {len(bar_keywords)}개)\")\n",
    "            plt.xlabel(\"TF-IDF 점수\")\n",
    "            plt.ylabel(\"단어\")\n",
    "            plt.tight_layout()\n",
    "            bar_path = f\"output/barplot/{(hotel)}_{suffix}.png\"\n",
    "            plt.savefig(bar_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f\"{hotel} ({'긍정' if label == 1 else '부정'}) 바 그래프 생략 - 키워드 부족\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4cf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_prop = fm.FontProperties(fname='NanumGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eec7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "숙소별 키워드 분석:   0%|          | 0/437 [00:00<?, ?it/s]c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "숙소별 키워드 분석:   0%|          | 1/437 [00:03<23:47,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전주 중앙동 라온 호텔 (긍정) 바 그래프 생략 - 키워드 부족\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "숙소별 키워드 분석:   0%|          | 2/437 [00:19<1:19:25, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전주 산정동 호텔 감스테이 (긍정) 바 그래프 생략 - 키워드 부족\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "숙소별 키워드 분석:   1%|          | 3/437 [00:29<1:14:06, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전주 중화산동 호텔 인트로(HOTEL INTRO) (긍정) 바 그래프 생략 - 키워드 부족\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "숙소별 키워드 분석:   1%|          | 4/437 [00:40<1:18:33, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전주 신시가지 호텔 팝 (긍정) 바 그래프 생략 - 키워드 부족\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\MYCOM\\AppData\\Local\\Temp\\ipykernel_6376\\858618710.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
      "숙소별 키워드 분석:   1%|          | 5/437 [00:55<1:29:21, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전주 산정동 호텔 레이나 (긍정) 바 그래프 생략 - 키워드 부족\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "숙소별 키워드 분석:   1%|          | 5/437 [00:57<1:22:16, 11.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m :\n\u001b[1;32m---> 39\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_sentiment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e :\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m오류 : \u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[48], line 33\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     31\u001b[0m     local_stopwords \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m좋다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m좋\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m좋습니다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m좋은\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m예쁘다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m깔끔하다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m깨끗하다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m친절하다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m편안하다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m따뜻하다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m만족하다\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_stopwords\u001b[49m\u001b[43m)\u001b[49m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 이하 기존 작업 동일\u001b[39;00m\n\u001b[0;32m     35\u001b[0m df_sentiment \u001b[38;5;241m=\u001b[39m df_hotel[df_hotel[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m label]\n",
      "Cell \u001b[1;32mIn[48], line 11\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(text, stopwords)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize\u001b[39m(text, stopwords\u001b[38;5;241m=\u001b[39m[]):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     10\u001b[0m             word\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m---> 11\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[43mokt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNoun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdjective\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m         ]\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenization error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\konlpy\\tag\\_okt.py:71\u001b[0m, in \u001b[0;36mOkt.pos\u001b[1;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"POS tagger.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mIn contrast to other classes in this subpackage,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mthis POS tagger doesn't have a `flatten` option,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m:param join: If True, returns joined sets of morph and tag.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m validate_phrase_inputs(phrase)\n\u001b[1;32m---> 71\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjki\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mphrase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBoolean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoArray()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m join:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----- 한글 폰트 설정 -----\n",
    "plt.rc('font', family='NanumGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ----- 토크나이저 -----\n",
    "okt = Okt()\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "# ----- 분석 시작 -----\n",
    "hotel_names = df['name'].unique()\n",
    "\n",
    "for hotel in tqdm(hotel_names, desc=\"숙소별 키워드 분석\") :\n",
    "    df_hotel = df[df['name'] == hotel]\n",
    "    accom_ids = df_hotel['accommodation_id'].value_counts()\n",
    "    accom_id = accom_ids.idxmax() if not accom_ids.empty else None\n",
    "\n",
    "    for label in [-1, 1]:\n",
    "        local_stopwords = stopwords.copy()\n",
    "        if label == -1:\n",
    "            local_stopwords += ['좋다', '좋', '좋습니다', '좋은', '예쁘다', '깔끔하다', '깨끗하다', '친절하다', '편안하다', '따뜻하다', '만족하다', '마을']\n",
    "        else :\n",
    "            local_stopwords += ['마을']\n",
    "        tfidf = TfidfVectorizer(tokenizer=lambda x: tokenize(x, stopwords=local_stopwords), max_features=1000)\n",
    "        # 이하 기존 작업 동일\n",
    "        df_sentiment = df_hotel[df_hotel['review_type'] == label]\n",
    "        if len(df_sentiment) < 5:\n",
    "            continue\n",
    "        try :\n",
    "            X = tfidf.fit_transform(df_sentiment['content'])\n",
    "        except Exception as e :\n",
    "            print(\"오류 : \", e)\n",
    "            continue\n",
    "        feature_names = np.array(tfidf.get_feature_names_out())\n",
    "        tfidf_mean = np.asarray(X.mean(axis=0)).ravel()\n",
    "        \n",
    "        topn = 20\n",
    "        top_idx = np.argsort(tfidf_mean)[::-1][:topn]\n",
    "        top_words = feature_names[top_idx]\n",
    "        top_scores = tfidf_mean[top_idx]\n",
    "        # 감정별 추가 불용어 설정\n",
    "\n",
    "        # TF-IDF\n",
    "        filtered = [(w, s) for w, s in zip(top_words, top_scores) if w.strip() not in local_stopwords]\n",
    "        bar_keywords = filtered[:20]\n",
    "        bar_words, bar_scores = zip(*bar_keywords) if bar_keywords else ([], [])\n",
    "\n",
    "        # 워드클라우드 저장\n",
    "        suffix = 'pos' if label == 1 else 'neg'\n",
    "        # 바 플롯 저장\n",
    "        df_keywords = pd.DataFrame({'word': bar_words, 'score': bar_scores})\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        base_palette = sns.color_palette(\"Blues\", n_colors=len(df_keywords)) if label == 1 else sns.color_palette(\"Reds\", n_colors=len(df_keywords))\n",
    "        sns.barplot(data=df_keywords, y='word', x='score', palette=list(reversed(base_palette)))\n",
    "        plt.title(f\"{hotel} {'긍정' if label == 1 else '부정'} 키워드 (상위 20개)\")\n",
    "        plt.xlabel(\"TF-IDF 점수\")\n",
    "        plt.ylabel(\"단어\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"output/barplot/{accom_id}_{suffix}.png\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"{hotel} ({'긍정' if label == 1 else '부정'}) 바 그래프 생략 - 키워드 부족\")\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_summary = []\n",
    "\n",
    "for hotel in tqdm(hotel_names, desc=\"숙소별 키워드 분석\") :\n",
    "    df_hotel = df[df['name'] == hotel]\n",
    "    accom_ids = df_hotel['accommodation_id'].value_counts()\n",
    "    accom_id = accom_ids.idxmax() if not accom_ids.empty else None\n",
    "\n",
    "    # 감성 점수 계산\n",
    "    pos_count = len(df_hotel[df_hotel['review_type'] == 1])\n",
    "    neg_count = len(df_hotel[df_hotel['review_type'] == -1])\n",
    "    total = pos_count + neg_count\n",
    "    if total > 0:\n",
    "        pos_percent = round((pos_count / total) * 100, 1)\n",
    "        neg_percent = round((neg_count / total) * 100, 1)\n",
    "        sentiment_score = round(pos_count / total, 3)\n",
    "    else:\n",
    "        pos_percent = neg_percent = sentiment_score = None\n",
    "\n",
    "    # 점수 저장\n",
    "    sentiment_summary.append({\n",
    "        'accommodation_id': accom_id,\n",
    "        'name': hotel,\n",
    "        'positive_count': pos_count,\n",
    "        'negative_count': neg_count,\n",
    "        'positive_percent': f\"{pos_percent}%\",\n",
    "        'negative_percent': f\"{neg_percent}%\",\n",
    "        'sentiment_score': sentiment_score\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(sentiment_summary)\n",
    "\n",
    "df = df.merge(summary_df, on='accommodation_id', how='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
