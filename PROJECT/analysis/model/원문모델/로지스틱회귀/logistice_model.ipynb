{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '부터', '며', '등', '하다', '한다', '하고', '하니',\n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '또한', '또는', '및', '즉', '한편', '반면에',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '싶다', '생각하다'\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96428718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MYCOM\\.conda\\envs\\azen\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Confusion Matrix ===\n",
      "[[2365  212]\n",
      " [ 263 2313]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.900     0.918     0.909      2577\n",
      "           1      0.916     0.898     0.907      2576\n",
      "\n",
      "    accuracy                          0.908      5153\n",
      "   macro avg      0.908     0.908     0.908      5153\n",
      "weighted avg      0.908     0.908     0.908      5153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. 데이터 불러오기 (중립 제거 포함)\n",
    "train_df = pd.read_csv(\"../data/train_data.csv\", encoding=\"utf-8-sig\")\n",
    "test_df = pd.read_csv(\"../data/test_data.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "train_df = train_df[train_df[\"label\"].isin([0, 1])]\n",
    "test_df = test_df[test_df[\"label\"].isin([0, 1])]\n",
    "\n",
    "X_train_text = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_text = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 2. 형태소 분석기 + tokenizer 함수 정의\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "tokenizer_with_stopwords = partial(tokenize, stopwords=stopwords)\n",
    "\n",
    "\n",
    "# 3. TF-IDF 벡터화 with tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer_with_stopwords, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "# 4. 모델 학습\n",
    "model = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f6a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  Accuracy  Precision(긍정)  Recall(긍정)    F1(긍정)  \\\n",
      "0      LogisticRegression  0.908791       0.915877    0.900233  0.907987   \n",
      "1  DecisionTreeClassifier  0.726373       0.803015    0.599767  0.686667   \n",
      "2  RandomForestClassifier  0.902581       0.894597    0.912655  0.903536   \n",
      "\n",
      "   Precision(부정)  Recall(부정)    F1(부정)  \n",
      "0       0.901946    0.917346  0.909581  \n",
      "1       0.680706    0.852930  0.757148  \n",
      "2       0.910891    0.892511  0.901607  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 모델 정의\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "logit = LogisticRegression(max_iter=1000)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#모델 훈련\n",
    "logit.fit(X_train, y_train)\n",
    "tree.fit(X_train, y_train)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "#models = [ logit , tree , forest ]\n",
    "results = test_df[['text', 'label']].copy()\n",
    "\n",
    "model_reports = []\n",
    "# 예측 및 정확도 평가\n",
    "for model in [logit, tree, forest]:\n",
    "    name = model.__class__.__name__\n",
    "    preds = model.predict(X_test)\n",
    "    report = classification_report(y_test, preds, target_names=[\"부정\", \"긍정\"], output_dict=True)\n",
    "    \n",
    "    model_reports.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, preds),\n",
    "        \"Precision(긍정)\": report[\"긍정\"][\"precision\"],\n",
    "        \"Recall(긍정)\": report[\"긍정\"][\"recall\"],\n",
    "        \"F1(긍정)\": report[\"긍정\"][\"f1-score\"],\n",
    "        \"Precision(부정)\": report[\"부정\"][\"precision\"],\n",
    "        \"Recall(부정)\": report[\"부정\"][\"recall\"],\n",
    "        \"F1(부정)\": report[\"부정\"][\"f1-score\"]\n",
    "    })\n",
    "    # 학습 및 예측 결과 저장\n",
    "    results[name] = preds\n",
    "\n",
    "report_df = pd.DataFrame(model_reports)\n",
    "results.to_csv('예측리뷰.csv', index=False, encoding='utf-8-sig')\n",
    "print(report_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c057802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_csv('model_성능.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "180f73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델과 벡터라이저 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델과 벡터라이저 저장\n",
    "joblib.dump(model, 'pkl/logistic_model.pkl')\n",
    "joblib.dump(vectorizer, 'pkl/logistic_tfdf_vectorizer.pkl')\n",
    "\n",
    "print(\"모델과 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b790eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ratings_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e98cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 2. 훈련/검증 분할\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# 3. 토크나이저 준비\n",
    "tokenizer = BertTokenizer.from_pretrained(\"skt/kobert-base-v1\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# 4. HuggingFace Dataset 객체로 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 5. 텐서 변환\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 6. 모델 로딩\n",
    "model = BertForSequenceClassification.from_pretrained(\"skt/kobert-base-v1\", num_labels=2)\n",
    "\n",
    "# 7. 훈련 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kobert_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# 8. 평가 함수 정의\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# 9. Trainer 구성 및 학습\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afdb23e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 1. 단어 리스트 및 가중치\u001b[39;00m\n\u001b[0;32m      6\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m----> 7\u001b[0m coef \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# 이진 분류이므로 shape (1, n_features)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. 긍정/부정 top 단어 인덱스 추출\u001b[39;00m\n\u001b[0;32m     10\u001b[0m topn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. 단어 리스트 및 가중치\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]  # 이진 분류이므로 shape (1, n_features)\n",
    "\n",
    "# 2. 긍정/부정 top 단어 인덱스 추출\n",
    "topn = 30\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "# 3. 긍정 / 부정 단어별 가중치 딕셔너리 생성\n",
    "word_weights = {\n",
    "    1: dict(zip(feature_names[top_pos_idx], coef[top_pos_idx])),\n",
    "    0: dict(zip(feature_names[top_neg_idx], coef[top_neg_idx])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8625098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import font_manager as fm\n",
    "font_path = \"C:/Windows/Fonts/NanumGothic.ttf\"\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=font_prop.get_name())\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def draw_wordcloud(word_weight_dict, title, color='Greens'):\n",
    "    wc = WordCloud(\n",
    "        font_path='C:/Windows/Fonts/NanumGothic.ttf',  # Mac은 AppleGothic, Linux는 나눔폰트\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94260d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_wordcloud(word_weights[0], '부정 키워드', color='Reds')\n",
    "draw_wordcloud(word_weights[1], '긍정 키워드', color='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986563c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 잠은 잘 잤는데 냄새났어요\n",
      "예측: 1, 확률: [0.45489311 0.54510689]\n",
      "문장: 서비스는 좋은데 시설이 별로였어요\n",
      "예측: 0, 확률: [0.54 0.46]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\"잠은 잘 잤는데 냄새났어요\", \"서비스는 좋은데 시설이 별로였어요\"]\n",
    "X_new = vectorizer.transform(new_texts)\n",
    "pred = model.predict(X_new)\n",
    "proba = model.predict_proba(X_new)\n",
    "\n",
    "\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    print(f\"문장: {text}\")\n",
    "    print(f\"예측: {pred[i]}, 확률: {proba[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 저장\n",
    "# joblib.dump(model, 'Logistic_model.pkl')\n",
    "# joblib.dump(vectorizer, 'Logistic_tfidf_vectorizer.pkl')\n",
    "# print(\"모델 및 벡터라이저 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 워드클라우드용 단어 + 가중치 딕셔너리 만들기\n",
    "word_weights = {\n",
    "    label: dict(zip(df['word'], df['weight']))\n",
    "    for label, df in weights.items()\n",
    "}\n",
    "print(word_weights.keys())\n",
    "\n",
    "# 워드클라우드 그리기 함수\n",
    "def draw_wordcloud(word_weight_dict, title, color):\n",
    "    wc = WordCloud(\n",
    "        font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf',\n",
    "        background_color='white',\n",
    "        colormap=color,\n",
    "        width=800,\n",
    "        height=400\n",
    "    )\n",
    "    wc.generate_from_frequencies(word_weight_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "# 클래스별 워드클라우드 출력\n",
    "# draw_wordcloud(word_weights[0], '부정 감성 주요 단어', 'Reds')\n",
    "# draw_wordcloud(word_weights[1], '긍정 감성 주요 단어', 'Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 문장 리스트\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/여기어때_리뷰.csv\")\n",
    "\n",
    "sentence = df['text']\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "labels = []\n",
    "threshold = 0.5\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold:\n",
    "        label = 1\n",
    "    elif prob_pos <= 1 - threshold:\n",
    "        label = -1\n",
    "    else:\n",
    "        label = 0\n",
    "    labels.append(label)\n",
    "\n",
    "df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_df = df[['name','rating','write_date','text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca044461",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_df.to_csv('예측리뷰.csv', encoding='utf-8-sig', index=False)\n",
    "nw_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/NanumGothic.ttf\"\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=font_prop.get_name())\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 6. 중요 단어 추출 (이진 분류는 coef_[0] 사용)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 20\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"부정 상위 단어 (label=0)\")\n",
    "axes[0].set_xlabel(\"가중치(weight)\")\n",
    "axes[0].set_ylabel(\"단어\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"긍정 상위 단어 (label=1)\")\n",
    "axes[1].set_xlabel(\"가중치(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from konlpy.tag import Okt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# 형태소 분석기 설정\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []\n",
    "\n",
    "# 그래프 한글 폰트 설정\n",
    "font_path = \"C:/Windows/Fonts/NanumGothic.ttf\"\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=font_prop.get_name())\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ----- 저장 폴더 -----\n",
    "os.makedirs(\"output/wordcloud\", exist_ok=True)\n",
    "os.makedirs(\"output/barplot\", exist_ok=True)\n",
    "\n",
    "def safe_filename(name):\n",
    "    return re.sub(r'[\\\\/:\"*?<>|]+', \"_\", name)\n",
    "\n",
    "if not {'name', 'clean_reviews', 'label'}.issubset(df.columns):\n",
    "    raise ValueError(\"'name', 'clean_reviews', 'label' 컬럼이 존재해야 합니다.\")\n",
    "\n",
    "# ----- 숙소별 반복 -----\n",
    "hotel_names = yeogi_df['name'].unique()\n",
    "\n",
    "for hotel in tqdm(hotel_names, desc=\"숙소별 키워드 분석\"):\n",
    "    df_hotel = yeogi_df[yeogi_df['name'] == hotel]\n",
    "\n",
    "    for label in [-1, 1]:  # 부정(0), 긍정(1)\n",
    "        df_sentiment = df_hotel[df_hotel['label'] == label]\n",
    "\n",
    "        if len(df_sentiment) < 5:\n",
    "            continue\n",
    "\n",
    "        tfidf = TfidfVectorizer(tokenizer=tokenize, max_features=1000)\n",
    "        X = tfidf.fit_transform(df_sentiment['clean_reviews'])\n",
    "        feature_names = np.array(tfidf.get_feature_names_out())\n",
    "        tfidf_mean = np.asarray(X.mean(axis=0)).ravel()\n",
    "\n",
    "        topn = 30\n",
    "        top_idx = np.argsort(tfidf_mean)[::-1][:topn]\n",
    "        top_words = feature_names[top_idx]\n",
    "        top_scores = tfidf_mean[top_idx]\n",
    "\n",
    "        filtered = [(w, s) for w, s in zip(top_words, top_scores) if w not in stopwords]\n",
    "        if label == -1 :\n",
    "            stopwords.append(['좋다', '예쁘다', '깔끔하다','깨끗하다', '친절하다', '편안하다', '깔끔하다', '따뜻하다'])\n",
    "        # 텍스트 토큰화 (전체 리뷰 합쳐서)\n",
    "        # 리스트를 다시 분리\n",
    "        top_words, top_scores = zip(*filtered) if filtered else ([], [])\n",
    "        suffix = 'pos' if label == 1 else 'neg'\n",
    "        # 워드클라우드 생성 및 저장\n",
    "        word_freq = dict(zip(top_words, top_scores))\n",
    "        wc = WordCloud(font_path=font_path, background_color='white', width=800, height=400)\n",
    "        wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "\n",
    "        wc_path = f\"output/wordcloud/{safe_filename(hotel)}_{suffix}.png\"\n",
    "        wc.to_file(wc_path)\n",
    "\n",
    "        # 바 그래프 저장\n",
    "        df_keywords = pd.DataFrame({'word': top_words, 'score': top_scores})\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(data=df_keywords, y='word', x='score', palette='Blues' if label== 1 else 'Reds')\n",
    "        plt.title(f\"{hotel} - {'긍정' if label== 1 else '부정'} 키워드\")\n",
    "        plt.xlabel(\"TF-IDF 점수\")\n",
    "        plt.ylabel(\"단어\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        bar_path = f\"output/barplot/{safe_filename(hotel)}_{suffix}.png\"\n",
    "        plt.savefig(bar_path)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
