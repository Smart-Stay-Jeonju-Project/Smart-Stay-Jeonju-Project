{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', '하다', '있다',\n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구', '그렇다',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다',\n",
    "    '했다', '같다', '네요','아니다',\n",
    "    '용하다', '물이', '뿐', '대로', '만', '따름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안','그냥',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 2. 형태소 분석기 + tokenizer 함수 정의\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(text, stopwords=[]):\n",
    "    try:\n",
    "        return [\n",
    "            word.lower()\n",
    "            for word, pos in okt.pos(text, stem=True)\n",
    "            if pos in ['Noun', 'Adjective', 'Verb']\n",
    "            and word.lower() not in stopwords\n",
    "            and len(word) > 1\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Tokenization error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288ff739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load('Logistic_model.pkl')\n",
    "vectorizer = joblib.load('Logistic_tfdf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 문장 리스트\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/new_test_data.csv\")\n",
    "\n",
    "sentence = df['text']\n",
    "\n",
    "# 벡터화 (학습한 vectorizer 사용)\n",
    "X_new = vectorizer.transform(sentence)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = model.predict(X_new)\n",
    "probs = model.predict_proba(X_new)\n",
    "\n",
    "labels = []\n",
    "threshold = 0.6\n",
    "for i, text in enumerate(sentence):\n",
    "    prob_pos = probs[i][1]\n",
    "    if prob_pos >= threshold:\n",
    "        label = 1\n",
    "    elif prob_pos <= 1 - threshold:\n",
    "        label = -1\n",
    "    else:\n",
    "        label = 0\n",
    "    labels.append(label)\n",
    "\n",
    "df['TF_label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 모델 정의\n",
    "logit = LogisticRegression(max_iter=1000)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# 모델 훈련\n",
    "# logit.fit(X_train, y_train)\n",
    "# tree.fit(X_train, y_train)\n",
    "# forest.fit(X_train, y_train)\n",
    "\n",
    "# # 예측 및 정확도 평가\n",
    "# for model in [logit, tree, forest]:\n",
    "#     preds = model.predict(X_test)\n",
    "#     print(model.__class__.__name__, \"정확도:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6507666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_df = df[['text','label','TF_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca044461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TF_label\n",
       "-1    6061\n",
       " 1    4643\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_df.to_csv('TF_test_data.csv', encoding='utf-8-sig', index=False)\n",
    "nw_df['TF_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import warnings\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/NanumGothic.ttf\"\n",
    "font_prop = fm.FontProperties(fname=font_path)\n",
    "plt.rc('font', family=font_prop.get_name())\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 6. 중요 단어 추출 (이진 분류는 coef_[0] 사용)\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = model.coef_[0]\n",
    "\n",
    "topn = 20\n",
    "top_pos_idx = np.argsort(coef)[::-1][:topn]\n",
    "top_neg_idx = np.argsort(coef)[:topn]\n",
    "\n",
    "df_pos = pd.DataFrame({'word': feature_names[top_pos_idx], 'weight': coef[top_pos_idx]})\n",
    "df_neg = pd.DataFrame({'word': feature_names[top_neg_idx], 'weight': coef[top_neg_idx]})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10), sharey=True)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=df_neg, y='word', x='weight', color='#e74c3c')\n",
    "axes[0].set_title(\"부정 상위 단어 (label=0)\")\n",
    "axes[0].set_xlabel(\"가중치(weight)\")\n",
    "axes[0].set_ylabel(\"단어\")\n",
    "\n",
    "sns.barplot(ax=axes[1], data=df_pos, y='word', x='weight', color='#2ecc71')\n",
    "axes[1].set_title(\"긍정 상위 단어 (label=1)\")\n",
    "axes[1].set_xlabel(\"가중치(weight)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. 데이터 로드\n",
    "df = pd.read_csv(\"keyword_test.csv\")  # 'clean_reviews'와 'label' 컬럼이 있어야 함\n",
    "df = df.dropna(subset=['clean_reviews', 'label'])  # 결측 제거\n",
    "df['label'] = df['label'].astype(int)  # 레이블 정수형 변환\n",
    "\n",
    "# 2. 데이터셋 분할\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# 3. 토크나이저 로드 (KoBERT)\n",
    "model_name = \"beomi/kcbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 4. 토큰화 함수 정의\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['clean_reviews'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# 5. HuggingFace Dataset으로 변환\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# 6. 토큰화 적용\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 7. label, input_ids, attention_mask만 추림\n",
    "train_dataset = train_dataset.remove_columns(['clean_reviews'])\n",
    "val_dataset = val_dataset.remove_columns(['clean_reviews'])\n",
    "\n",
    "# 8. PyTorch tensor 형식 명시\n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "# 9. 모델 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# 10. 평가지표 함수\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted')\n",
    "    }\n",
    "\n",
    "# 11. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_sentiment_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 12. Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 13. 학습 실행\n",
    "trainer.train()\n",
    "\n",
    "# 14. 모델 저장\n",
    "trainer.save_model(\"./saved_bert_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./saved_bert_sentiment_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
