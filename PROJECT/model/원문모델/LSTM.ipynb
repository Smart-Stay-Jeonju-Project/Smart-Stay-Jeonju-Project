{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c584315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # í•œê¸€, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°\n",
    "    text = re.sub(r\"[^ê°€-í£0-9\\s]\", \"\", str(text))\n",
    "    words = text.split()\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a25aefff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›Œë“œ ì¸ë±ìŠ¤ ë‹¨ì–´ ê°œìˆ˜ : 5662\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"max_word_index.json\",\"r\", encoding='utf-8') as f:\n",
    "    word_index = json.load(f)\n",
    "print(f\"ì›Œë“œ ì¸ë±ìŠ¤ ë‹¨ì–´ ê°œìˆ˜ : {len(word_index)}\")\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b4cd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ì˜ í•˜ì´í¼ íŒŒë¼ë©”íƒ€ì¸ ì„ë² ë”© ë°±í„°\n",
    "embedding_dim = 128\n",
    "hidden_units = 128\n",
    "# ìƒ˜í”Œì˜ ì›ì†Œ ê°œìˆ˜ ì œí•œ\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e47fe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "model = Sequential()\n",
    "# ì…ë ¥ì¸µ ì„¤ì •\n",
    "model.add(Embedding(\n",
    "    # ì…ë ¥ ë‹¨ì–´ì˜ ì¢…ë¥˜ ìˆ˜ : ë‹¨ì–´ ì‚¬ì „ì˜ ë‹¨ì–´ ê°œìˆ˜\n",
    "    vocab_size,             # ì •ìˆ˜ ì¸ë±ìŠ¤ì˜ ìµœëŒ€ ê°’\n",
    "    # ê° ë‹¨ì–´ë¥¼ ëª‡ê°œì˜ íŠ¹ì„±ìœ¼ë¡œ í‘œí˜„í•  ê²ƒì¸ê°€\n",
    "    embedding_dim,          # ì„ë² ë”© ë°±í„° ì°¨ì› ê°’\n",
    "    input_length=max_length # í›ˆë ¨ ë°ì´í„°ì˜ íŠ¹ì„± ê°œìˆ˜ : ìƒ˜í”Œì˜ ì›ì†Œ ê°œìˆ˜\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "329049c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# í•™ìŠµ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "with open(\"LSTM_model/X_train_sequences.pickle\",\"rb\") as fr:\n",
    "    X_train = pickle.load(fr)\n",
    "with open(\"LSTM_model/X_test_sequences.pickle\",\"rb\") as fr:\n",
    "    X_test = pickle.load(fr)\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "with open(\"LSTM_model/y_train_filterd.pickle\",\"rb\") as fr:\n",
    "    y_train = pickle.load(fr)\n",
    "with open(\"LSTM_model/y_test_filterd.pickle\",\"rb\") as fr:\n",
    "    y_test = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c06339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œëœ ë°ì´í„° ê°œìˆ˜ :\n",
      "X_train : 962 / y_train : 6100\n",
      "X_test : 979 / y_test : 1966\n"
     ]
    }
   ],
   "source": [
    "print(\"ë¡œë“œëœ ë°ì´í„° ê°œìˆ˜ :\")\n",
    "print(f\"X_train : {len(X_train)} / y_train : {len(y_train)}\")\n",
    "print(f\"X_test : {len(X_test)} / y_test : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d593d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. ì›ë˜ í…ìŠ¤íŠ¸ (str) ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”\n",
    "train_df = pd.read_csv(\"ratings_train.csv\")\n",
    "X_train_raw = train_df['review_content'].astype(str).apply(clean_text).tolist()\n",
    "\n",
    "# 2. í† í¬ë‚˜ì´ì € í•™ìŠµ\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_raw)\n",
    "\n",
    "# 3. ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_raw)\n",
    "\n",
    "# 4. ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”©\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post', truncating='post', dtype='int32')\n",
    "\n",
    "# 5. ë¼ë²¨\n",
    "y_train = train_df['label'].astype(np.int32).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^ê°€-í£0-9\\s]\", \"\", str(text))\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_df = pd.read_csv(\"ratings_train.csv\")\n",
    "test_df = pd.read_csv(\"ratings_test.csv\")\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "train_df['review_content'] = train_df['review_content'].astype(str).apply(clean_text)\n",
    "test_df['review_content'] = test_df['review_content'].astype(str).apply(clean_text)\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
    "X_train = train_df['review_content'].tolist()\n",
    "X_test = test_df['review_content'].tolist()\n",
    "\n",
    "# ë¼ë²¨ ë¦¬ìŠ¤íŠ¸\n",
    "y_train = train_df['label'].astype(np.int32).values\n",
    "y_test = test_df['label'].astype(np.int32).values\n",
    "\n",
    "# 4. í† í¬ë‚˜ì´ì € í•™ìŠµ ë° ì •ìˆ˜ ì¸ì½”ë”©\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 5. ê¸¸ì´ ì œí•œ í›„ ë‹¤ì‹œ ìŒ ì •ë ¬\n",
    "def trim_samples(X, y, max_len):\n",
    "    X_out, y_out = [], []\n",
    "    for x, label in zip(X, y):\n",
    "        if len(x) <= max_len:\n",
    "            X_out.append(x)\n",
    "            y_out.append(label)\n",
    "    return X_out, y_out\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ë³€í™˜\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# íŒ¨ë”©\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54ba0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¼ë²¨ 0 ê°œìˆ˜: 4800\n",
      "ë¼ë²¨ 1 ê°œìˆ˜: 4800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"ë¼ë²¨ {label} ê°œìˆ˜: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f13ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5117\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52812, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 6s 64ms/step - loss: 0.6931 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.5281\n",
      "Epoch 2/12\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6928 - accuracy: 0.5089\n",
      "Epoch 2: val_accuracy improved from 0.52812 to 0.53125, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.6928 - accuracy: 0.5089 - val_loss: 0.6925 - val_accuracy: 0.5312\n",
      "Epoch 3/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6927 - accuracy: 0.5151\n",
      "Epoch 3: val_accuracy improved from 0.53125 to 0.53177, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.6927 - accuracy: 0.5156 - val_loss: 0.6922 - val_accuracy: 0.5318\n",
      "Epoch 4/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6922 - accuracy: 0.5254\n",
      "Epoch 4: val_accuracy did not improve from 0.53177\n",
      "60/60 [==============================] - 3s 51ms/step - loss: 0.6921 - accuracy: 0.5271 - val_loss: 0.6917 - val_accuracy: 0.5318\n",
      "Epoch 5/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6918 - accuracy: 0.5185\n",
      "Epoch 5: val_accuracy improved from 0.53177 to 0.53229, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.6918 - accuracy: 0.5194 - val_loss: 0.6911 - val_accuracy: 0.5323\n",
      "Epoch 6/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.5293\n",
      "Epoch 6: val_accuracy improved from 0.53229 to 0.53594, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 55ms/step - loss: 0.6910 - accuracy: 0.5298 - val_loss: 0.6902 - val_accuracy: 0.5359\n",
      "Epoch 7/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.5253\n",
      "Epoch 7: val_accuracy improved from 0.53594 to 0.53750, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.6903 - accuracy: 0.5255 - val_loss: 0.6884 - val_accuracy: 0.5375\n",
      "Epoch 8/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5470\n",
      "Epoch 8: val_accuracy improved from 0.53750 to 0.62552, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.6854 - accuracy: 0.5474 - val_loss: 0.6640 - val_accuracy: 0.6255\n",
      "Epoch 9/12\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.6757\n",
      "Epoch 9: val_accuracy improved from 0.62552 to 0.86615, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 53ms/step - loss: 0.6304 - accuracy: 0.6757 - val_loss: 0.4816 - val_accuracy: 0.8661\n",
      "Epoch 10/12\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9206\n",
      "Epoch 10: val_accuracy improved from 0.86615 to 0.92865, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.3383 - accuracy: 0.9206 - val_loss: 0.2554 - val_accuracy: 0.9286\n",
      "Epoch 11/12\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.8986\n",
      "Epoch 11: val_accuracy did not improve from 0.92865\n",
      "60/60 [==============================] - 3s 54ms/step - loss: 0.3287 - accuracy: 0.8986 - val_loss: 0.3296 - val_accuracy: 0.8917\n",
      "Epoch 12/12\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.2741 - accuracy: 0.9243\n",
      "Epoch 12: val_accuracy improved from 0.92865 to 0.93073, saving model to ./logs/exp_20250715-113702_lr0.0001_bs64_lstm64\\best_model.h5\n",
      "60/60 [==============================] - 3s 52ms/step - loss: 0.2731 - accuracy: 0.9246 - val_loss: 0.2497 - val_accuracy: 0.9307\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils import class_weight\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# ì‹¤í—˜ ì´ë¦„ ì •ì˜ (ë‚ ì§œ/ì‹œê°„ + ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°)\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "experiment_name = f\"exp_{timestamp}_lr{learning_rate}_bs64_lstm64\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "log_dir = f\"./logs/{experiment_name}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# CSVLogger: epochë§ˆë‹¤ val_acc, loss ë“± ê¸°ë¡\n",
    "csv_logger = CSVLogger(os.path.join(log_dir, 'training_log.csv'))\n",
    "\n",
    "# EarlyStopping & ModelCheckpoint (ê°™ì´ ì‚¬ìš©)\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "mc = ModelCheckpoint(os.path.join(log_dir, 'best_model.h5'),\n",
    "                     monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì„±\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dropout(0.6),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=learning_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# í›ˆë ¨\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=12,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[es, mc, csv_logger],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "\n",
    "# history ê²°ê³¼ì—ì„œ ë§ˆì§€ë§‰ ì„±ëŠ¥ ìš”ì•½ ì €ì¥\n",
    "final_log_path = os.path.join(log_dir, 'summary.csv')\n",
    "with open(final_log_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['param', 'value'])\n",
    "    writer.writerow(['timestamp', timestamp])\n",
    "    writer.writerow(['embedding_dim', embedding_dim])\n",
    "    writer.writerow(['lstm_units', hidden_units])\n",
    "    writer.writerow(['learning_rate', learning_rate])\n",
    "    writer.writerow(['batch_size', BATCH_SIZE])\n",
    "    writer.writerow(['max_len', max_len])\n",
    "    writer.writerow(['val_accuracy_last', history.history['val_accuracy'][-1]])\n",
    "    writer.writerow(['val_loss_last', history.history['val_loss'][-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    pad = pad_sequences(seq, maxlen=100, padding='post', truncating='post')\n",
    "    prob = model.predict(pad)[0][0]\n",
    "    label = 'ê¸ì •' if prob > 0.5 else 'ë¶€ì •'\n",
    "    print(f\"[ë¦¬ë·°] {text}\\nâ†’ ì˜ˆì¸¡: {label} (í™•ë¥ : {prob:.2f})\")\n",
    "    return label, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3168a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"ì‚¬ì¥ë‹˜ì´ ì •ë§ ì¹œì ˆí•˜ê³  ë°©ë„ ê¹¨ë—í•´ìš”!\")\n",
    "predict_sentiment(\"ë°©ìŒì´ ë„ˆë¬´ ì•ˆë˜ê³  ì‹œì„¤ì´ ë‚¡ì•˜ì–´ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3291da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = os.path.join(log_dir, 'final_model.keras')\n",
    "model.save(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# âœ… 1. ë°ì´í„° ë¡œë”©\n",
    "df = pd.read_csv(\"train_data.csv\")  # ë°˜ë“œì‹œ text, label ì»¬ëŸ¼ ìˆì–´ì•¼ í•¨\n",
    "df = df[['text', 'label']]  # label: 0=ë¶€ì •, 1=ê¸ì •\n",
    "\n",
    "# âœ… 2. í•™ìŠµ/ê²€ì¦ ë¶„í• \n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# âœ… 3. Huggingface Datasets ë³€í™˜\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# âœ… 4. Tokenizer ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "model_name = \"klue/roberta-base\"  # ë˜ëŠ” \"monologg/kobert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# âœ… 5. ëª¨ë¸ ì •ì˜ (2 í´ë˜ìŠ¤ ë¶„ë¥˜)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# âœ… 6. í›ˆë ¨ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# âœ… 7. í‰ê°€ ì§€í‘œ ì •ì˜\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = np.mean(preds == labels)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# âœ… 8. Trainer ê°ì²´ ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# âœ… 9. ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# âœ… 10. ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "preds = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "true_labels = preds.label_ids\n",
    "\n",
    "print(\"\\nğŸ“Š í‰ê°€ ê²°ê³¼:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"ë¶€ì •\", \"ê¸ì •\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ef673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss, acc = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"âœ… Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥\n",
    "for i in range(5):\n",
    "    print(f\"ğŸ“ {X_test[i]}\")\n",
    "    print(f\"âœ… ì‹¤ì œ: {y_test[i]} / ì˜ˆì¸¡: {y_pred[i][0]}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"best_model_keras.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ee8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥  â†’ 0.5 ê¸°ì¤€ ì´ì§„ ë¶„ë¥˜\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# ê²°ê³¼ DataFrame ìƒì„±\n",
    "results_df = pd.DataFrame({\n",
    "    'text': X_test,  # ì›ë¬¸ í…ìŠ¤íŠ¸\n",
    "    'true_label': y_test,\n",
    "    'pred_label': y_pred,\n",
    "    'pred_prob': y_pred_probs.flatten()\n",
    "})\n",
    "\n",
    "# ì €ì¥\n",
    "results_df.to_csv(\"lstm_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ lstm_predictions.csvì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24942a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe594c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def extract_nouns(texts):\n",
    "    from konlpy.tag import Okt\n",
    "    okt = Okt()\n",
    "    all_nouns = []\n",
    "    for text in texts:\n",
    "        nouns = okt.nouns(str(text))\n",
    "        nouns = [n for n in nouns if len(n) > 1]  # 1ê¸€ì ì œì™¸\n",
    "        all_nouns.extend(nouns)\n",
    "    return all_nouns\n",
    "\n",
    "# ì˜ˆì¸¡ ê¸°ì¤€ ë¶„ë¦¬\n",
    "positive_texts = results_df[results_df['pred_label'] == 1]['text'].tolist()\n",
    "negative_texts = results_df[results_df['pred_label'] == 0]['text'].tolist()\n",
    "\n",
    "# í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "positive_nouns = extract_nouns(positive_texts)\n",
    "negative_nouns = extract_nouns(negative_texts)\n",
    "\n",
    "# ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "pos_freq = Counter(positive_nouns)\n",
    "neg_freq = Counter(negative_nouns)\n",
    "\n",
    "def generate_wordcloud(freq_dict, title, font_path='NanumGothic.ttf'):\n",
    "    wc = WordCloud(\n",
    "        font_path=font_path,\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white'\n",
    "    )\n",
    "    wc_img = wc.generate_from_frequencies(freq_dict)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wc_img, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(pos_freq, \"pos\")\n",
    "generate_wordcloud(neg_freq, \"neg\")\n",
    "\n",
    "wc = WordCloud(font_path='NanumGothic.ttf', width=800, height=400, background_color='white')\n",
    "wc.generate_from_frequencies(pos_freq).to_file(\"positive_wordcloud.png\")\n",
    "wc.generate_from_frequencies(neg_freq).to_file(\"negative_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16261b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac108d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fca3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3c6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model_keras.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "# model = load_model(\"best_model_keras.keras\")\n",
    "# model = load_model(\"best_model.h5\", compile=False)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "import pickle\n",
    "with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: ì‹¤ì œ ë¦¬ë·°)\n",
    "df = pd.read_csv(\"y_reviews.csv\")\n",
    "texts = df['review_content'].tolist()\n",
    "\n",
    "# ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ë³€í™˜\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "padded = pad_sequences(sequences, maxlen=30)  # maxlenì€ í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ê°’ê³¼ ë™ì¼í•˜ê²Œ\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "probs = model.predict(padded)\n",
    "preds = (probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# ê²°ê³¼ ì¶”ê°€\n",
    "df['predicted_label'] = preds\n",
    "df['sentiment_score'] = probs.flatten()  # 0~1 ê°ì„± ì ìˆ˜\n",
    "\n",
    "# ì €ì¥\n",
    "df.to_csv(\"ì˜ˆì¸¡ëœ_ê°ì„±_ë¦¬ë·°.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['sentiment_score'], bins=50)\n",
    "plt.title(\"ê°ì„± ì ìˆ˜ ë¶„í¬\")\n",
    "plt.xlabel(\"sentiment_score\")\n",
    "plt.ylabel(\"ë¦¬ë·° ìˆ˜\")\n",
    "plt.axvline(0.5, color='gray', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47df2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_3way(score, center=0.5, neutral_margin=0.01):\n",
    "    if abs(score - center) < neutral_margin:\n",
    "        return \"ì¤‘ë¦½\"\n",
    "    elif score >= center + neutral_margin:\n",
    "        return \"ê¸ì •\"\n",
    "    else:\n",
    "        return \"ë¶€ì •\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_label_3way'] = df['sentiment_score'].apply(classify_3way)\n",
    "df.to_csv(\"ì˜ˆì¸¡ëœ_ê°ì„±_ë¦¬ë·°.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
