{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆìš©ì–´ ëª©ë¡ ì„ ì–¸\n",
    "stopwords = list(set([\n",
    "    'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'ê»˜', 'ë¡œ', 'ìœ¼ë¡œ', \n",
    "    'ì™€', 'ê³¼', 'ë³´ë‹¤', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ê°™ì´', 'ê¹Œì§€', 'ë§ˆì €', 'ì¡°ì°¨', 'ë¶€í„°', \n",
    "    'ì´ë‚˜', 'ë‚˜', 'ì´ë©°', 'ë©°', 'ë“±', 'í•˜ë‹¤', 'í•œë‹¤', 'í•˜ê³ ', 'í•˜ë‹ˆ', 'í•˜ë©´', \n",
    "    'ë˜ì–´', 'ë˜ë‹¤', 'ë˜ê³ ', 'ë˜ë‹ˆ', 'ì…ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ã…‚ë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ë‹¤',\n",
    "    'ê³ ', 'ë©´', 'ê²Œ', 'ì§€', 'ì£ ',\n",
    "    'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë©´', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë”°ë¼ì„œ', \n",
    "    'ë˜í•œ', 'ë˜ëŠ”', 'ë°', 'ì¦‰', 'í•œí¸', 'ë°˜ë©´ì—', 'ê·¼ë°',\n",
    "    'ë‚˜', 'ì €', 'ìš°ë¦¬', 'ì €í¬', 'ë„ˆ', 'ë„ˆí¬', 'ë‹¹ì‹ ', 'ê·¸', 'ê·¸ë…€', 'ê·¸ë“¤', 'ëˆ„êµ¬',\n",
    "    'ë¬´ì—‡', 'ì–´ë””', 'ì–¸ì œ', 'ì–´ëŠ', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', \n",
    "    'ì´ìª½', 'ê·¸ìª½', 'ì €ìª½',\n",
    "    'í•˜ë‚˜', 'ë‘˜', 'ì…‹', 'ë„·', 'ë‹¤ì„¯', 'ì—¬ì„¯', 'ì¼ê³±', 'ì—¬ëŸ', 'ì•„í™‰', 'ì—´',\n",
    "    'ì¼', 'ì´', 'ì‚¼', 'ì‚¬', 'ì˜¤', 'ìœ¡', 'ì¹ ', 'íŒ”', 'êµ¬', 'ì‹­', 'ë°±', 'ì²œ', 'ë§Œ',\n",
    "    'ì²«ì§¸', 'ë‘˜ì§¸', 'ì…‹ì§¸',\n",
    "    'ë°”ë¡œ', 'ë•Œ', 'ê²ƒ', 'ìˆ˜', 'ë¬¸ì œ', 'ê²½ìš°', 'ë¶€ë¶„', 'ì´ë‹¤',\n",
    "    'ë‚´ìš©', 'ê²°ê³¼', 'ìì²´', 'ê°€ì§€', 'ìˆë‹¤',\n",
    "    'ì•Šì•˜ì–´ìš”', 'ìˆì—ˆì–´ìš”', 'í–ˆì–´ìš”', 'í–ˆëŠ”ë°ìš”', 'ìˆëŠ”ë°ìš”', 'í•©ë‹ˆë‹¤', 'ì—†ë‹¤', 'ë‚˜ë‹¤','ìƒê°í•˜ë‹¤',\n",
    "    'í–ˆë‹¤', 'ê°™ë‹¤', 'ë„¤ìš”','ì•„ë‹ˆë‹¤', 'ìš©í•˜ë‹¤', 'ë¬¼ì´',\n",
    "    'ë¿', 'ëŒ€ë¡œ', 'ë§Œ', 'ë”°ë¦„', 'ê¹€ì—', 'í„°',\n",
    "    'ì•„', 'ì•„ì´ê³ ', 'ì•„ì´êµ¬', 'ì•„í•˜', 'ì–´', 'ê·¸ë˜', 'ì‘', 'ë„¤', 'ì˜ˆ', 'ì•„ë‹ˆ', 'ì•Šë‹¤', 'ì•ˆë˜ë‹¤','ì•ˆ',\n",
    "    'ê°€ë‹¤', 'ì˜¤ë‹¤', 'ì£¼ë‹¤', 'ë§ë‹¤', 'ë‚˜ë‹¤', 'ë°›ë‹¤', 'ì•Œë‹¤', 'ëª¨ë¥´ë‹¤', 'ì‹¶ë‹¤', 'ìƒê°í•˜ë‹¤', 'ë“¤ë‹¤'\n",
    "]))\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e03577",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pickle', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('X_test.pickle', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('y_train.pickle', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('y_test.pickle', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data, stopwords):\n",
    "    return [[word for word in sentence if word not in stopwords] for sentence in data]\n",
    "\n",
    "X_train = remove_stopwords(X_train, stopwords)\n",
    "X_test = remove_stopwords(X_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(X, y):\n",
    "    return zip(*[(x, label) for x, label in zip(X, y) if len(x) > 0])\n",
    "\n",
    "X_train, y_train = remove_empty(X_train, y_train)\n",
    "X_test, y_test = remove_empty(X_test, y_test)\n",
    "\n",
    "X_train, y_train = list(X_train), list(y_train)\n",
    "X_test, y_test = list(X_test), list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def trim_samples(X, y, max_len):\n",
    "    return zip(*[(x, label) for x, label in zip(X, y) if len(x) <= max_len])\n",
    "\n",
    "X_train, y_train = trim_samples(X_train, y_train, max_len)\n",
    "X_test, y_test = trim_samples(X_test, y_test, max_len)\n",
    "\n",
    "X_train, y_train = list(X_train), list(y_train)\n",
    "X_test, y_test = list(X_test), list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "total_count = len(tokenizer.word_index)\n",
    "threshold = 3\n",
    "rare_count = sum(1 for word, freq in tokenizer.word_counts.items() if freq < threshold)\n",
    "rare_freq = sum(freq for word, freq in tokenizer.word_counts.items() if freq < threshold)\n",
    "total_freq = sum(tokenizer.word_counts.values())\n",
    "\n",
    "vocab_size = total_count - rare_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# ì •ìˆ˜ ì¸ì½”ë”©\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# ë¹ˆ ìƒ˜í”Œ ì œê±°\n",
    "X_train, y_train = zip(*[(x, y) for x, y in zip(X_train, y_train) if len(x) > 0])\n",
    "X_test, y_test = zip(*[(x, y) for x, y in zip(X_test, y_test) if len(x) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "pad_X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print(f\"\\nâœ… ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(pad_X_train)}ê°œ\")\n",
    "print(f\"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(pad_X_test)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_sequences.pickle', 'wb') as fw:\n",
    "    pickle.dump(pad_X_train, fw)\n",
    "with open('X_test_sequences.pickle', 'wb') as fw:\n",
    "    pickle.dump(pad_X_test, fw)\n",
    "with open('y_train_filterd.pickle', 'wb') as fw:\n",
    "    pickle.dump(list(y_train), fw)\n",
    "with open('y_test_filterd.pickle', 'wb') as fw:\n",
    "    pickle.dump(list(y_test), fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_word_index = {word: idx for word, idx in tokenizer.word_index.items() if idx < vocab_size}\n",
    "\n",
    "with open('max_word_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(limited_word_index, f, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nğŸ“š ë‹¨ì–´ ì‚¬ì „ ì €ì¥ ì™„ë£Œ ({len(limited_word_index)}ê°œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6493504",
   "metadata": {},
   "source": [
    "145393 / 145162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
