{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e7dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 목록 선언\n",
    "stopwords = list(set([\n",
    "    '이', '가', '은', '는', '을', '를', '의', '에', '에서', '에게', '께', '로', '으로', \n",
    "    '와', '과', '보다', '처럼', '만큼', '같이', '까지', '마저', '조차', '부터', \n",
    "    '이나', '나', '이며', '며', '등', '하다', '한다', '하고', '하니', '하면', \n",
    "    '되어', '되다', '되고', '되니', '입니다', '습니다', 'ㅂ니다', '어요', '아요', '다',\n",
    "    '고', '면', '게', '지', '죠',\n",
    "    '그리고', '그러나', '하지만', '그런데', '그래서', '그러면', '그러므로', '따라서', \n",
    "    '또한', '또는', '및', '즉', '한편', '반면에', '근데',\n",
    "    '나', '저', '우리', '저희', '너', '너희', '당신', '그', '그녀', '그들', '누구',\n",
    "    '무엇', '어디', '언제', '어느', '이것', '그것', '저것', '여기', '거기', '저기', \n",
    "    '이쪽', '그쪽', '저쪽',\n",
    "    '하나', '둘', '셋', '넷', '다섯', '여섯', '일곱', '여덟', '아홉', '열',\n",
    "    '일', '이', '삼', '사', '오', '육', '칠', '팔', '구', '십', '백', '천', '만',\n",
    "    '첫째', '둘째', '셋째',\n",
    "    '바로', '때', '것', '수', '문제', '경우', '부분', '이다',\n",
    "    '내용', '결과', '자체', '가지', '있다',\n",
    "    '않았어요', '있었어요', '했어요', '했는데요', '있는데요', '합니다', '없다', '나다','생각하다',\n",
    "    '했다', '같다', '네요','아니다', '용하다', '물이',\n",
    "    '뿐', '대로', '만', '따름', '김에', '터',\n",
    "    '아', '아이고', '아이구', '아하', '어', '그래', '응', '네', '예', '아니', '않다', '안되다','안',\n",
    "    '가다', '오다', '주다', '말다', '나다', '받다', '알다', '모르다', '싶다', '생각하다', '들다'\n",
    "]))\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e03577",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train.pickle', 'rb') as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open('X_test.pickle', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open('y_train.pickle', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open('y_test.pickle', 'rb') as f:\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(data, stopwords):\n",
    "    return [[word for word in sentence if word not in stopwords] for sentence in data]\n",
    "\n",
    "X_train = remove_stopwords(X_train, stopwords)\n",
    "X_test = remove_stopwords(X_test, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty(X, y):\n",
    "    return zip(*[(x, label) for x, label in zip(X, y) if len(x) > 0])\n",
    "\n",
    "X_train, y_train = remove_empty(X_train, y_train)\n",
    "X_test, y_test = remove_empty(X_test, y_test)\n",
    "\n",
    "X_train, y_train = list(X_train), list(y_train)\n",
    "X_test, y_test = list(X_test), list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c073e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "def trim_samples(X, y, max_len):\n",
    "    return zip(*[(x, label) for x, label in zip(X, y) if len(x) <= max_len])\n",
    "\n",
    "X_train, y_train = trim_samples(X_train, y_train, max_len)\n",
    "X_test, y_test = trim_samples(X_test, y_test, max_len)\n",
    "\n",
    "X_train, y_train = list(X_train), list(y_train)\n",
    "X_test, y_test = list(X_test), list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae0a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "total_count = len(tokenizer.word_index)\n",
    "threshold = 3\n",
    "rare_count = sum(1 for word, freq in tokenizer.word_counts.items() if freq < threshold)\n",
    "rare_freq = sum(freq for word, freq in tokenizer.word_counts.items() if freq < threshold)\n",
    "total_freq = sum(tokenizer.word_counts.values())\n",
    "\n",
    "vocab_size = total_count - rare_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 정수 인코딩\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 빈 샘플 제거\n",
    "X_train, y_train = zip(*[(x, y) for x, y in zip(X_train, y_train) if len(x) > 0])\n",
    "X_test, y_test = zip(*[(x, y) for x, y in zip(X_test, y_test) if len(x) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "pad_X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "print(f\"\\n✅ 최종 학습 데이터: {len(pad_X_train)}개\")\n",
    "print(f\"✅ 최종 테스트 데이터: {len(pad_X_test)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c45fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_sequences.pickle', 'wb') as fw:\n",
    "    pickle.dump(pad_X_train, fw)\n",
    "with open('X_test_sequences.pickle', 'wb') as fw:\n",
    "    pickle.dump(pad_X_test, fw)\n",
    "with open('y_train_filterd.pickle', 'wb') as fw:\n",
    "    pickle.dump(list(y_train), fw)\n",
    "with open('y_test_filterd.pickle', 'wb') as fw:\n",
    "    pickle.dump(list(y_test), fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73aa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_word_index = {word: idx for word, idx in tokenizer.word_index.items() if idx < vocab_size}\n",
    "\n",
    "with open('max_word_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(limited_word_index, f, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n📚 단어 사전 저장 완료 ({len(limited_word_index)}개)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6493504",
   "metadata": {},
   "source": [
    "145393 / 145162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
